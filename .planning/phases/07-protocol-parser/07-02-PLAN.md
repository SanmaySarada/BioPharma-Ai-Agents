---
phase: 07-protocol-parser
plan: 02
type: execute
wave: 2
depends_on: ["07-01"]
files_modified:
  - src/omni_agents/llm/base.py
  - src/omni_agents/llm/gemini.py
  - src/omni_agents/llm/openai_adapter.py
  - src/omni_agents/agents/protocol_parser.py
  - src/omni_agents/templates/prompts/protocol_parser.j2
autonomous: true

must_haves:
  truths:
    - "BaseLLM has a generate_structured() method that returns a Pydantic model instance"
    - "GeminiAdapter.generate_structured() uses response_schema + response_mime_type='application/json'"
    - "OpenAIAdapter.generate_structured() uses client.beta.chat.completions.parse() with response_format"
    - "ProtocolParserAgent.parse() reads .docx, calls LLM, returns ExtractionResult"
    - "The prompt template enumerates all 14 TrialConfig fields with descriptions, expected ranges, and extraction rules"
  artifacts:
    - path: "src/omni_agents/llm/base.py"
      provides: "generate_structured() abstract method on BaseLLM"
      contains: "async def generate_structured"
    - path: "src/omni_agents/llm/gemini.py"
      provides: "Gemini structured output implementation"
      contains: "response_mime_type"
    - path: "src/omni_agents/llm/openai_adapter.py"
      provides: "OpenAI structured output implementation"
      contains: "response_format"
    - path: "src/omni_agents/agents/protocol_parser.py"
      provides: "ProtocolParserAgent class"
      contains: "class ProtocolParserAgent"
    - path: "src/omni_agents/templates/prompts/protocol_parser.j2"
      provides: "Extraction prompt with field descriptions and ranges"
      contains: "n_subjects"
  key_links:
    - from: "src/omni_agents/agents/protocol_parser.py"
      to: "src/omni_agents/llm/base.py"
      via: "self.llm.generate_structured()"
      pattern: "generate_structured"
    - from: "src/omni_agents/agents/protocol_parser.py"
      to: "src/omni_agents/agents/docx_reader.py"
      via: "extract_protocol_text()"
      pattern: "extract_protocol_text"
    - from: "src/omni_agents/agents/protocol_parser.py"
      to: "src/omni_agents/config.py"
      via: "merge_extraction()"
      pattern: "merge_extraction"
---

<objective>
Add structured output capability to the LLM adapters and build the ProtocolParserAgent class with its extraction prompt template.

Purpose: The protocol parser needs to call an LLM and receive typed Pydantic output (not raw text). Both Gemini and OpenAI SDKs support this natively via response_schema/response_format. The agent class orchestrates: read .docx -> call LLM with structured output -> merge with defaults -> return ExtractionResult.

Output: Updated BaseLLM/GeminiAdapter/OpenAIAdapter with generate_structured(), new ProtocolParserAgent class, new protocol_parser.j2 template.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/research/STACK.md (generate_structured patterns for both SDKs, ProtocolExtraction as response_model)
@.planning/research/ARCHITECTURE.md (ProtocolParserAgent design, NOT a BaseAgent subclass)
@.planning/research/PITFALLS.md (PITFALL-01 silent misextraction, PITFALL-08 prompt engineering)

@.planning/phases/07-protocol-parser/07-01-SUMMARY.md

@src/omni_agents/llm/base.py (BaseLLM -- add generate_structured)
@src/omni_agents/llm/gemini.py (GeminiAdapter -- implement generate_structured)
@src/omni_agents/llm/openai_adapter.py (OpenAIAdapter -- implement generate_structured)
@src/omni_agents/agents/base.py (BaseAgent pattern -- do NOT inherit from this)
@src/omni_agents/config.py (TrialConfig, ProtocolExtraction, merge_extraction from Plan 01)
@src/omni_agents/agents/docx_reader.py (extract_protocol_text from Plan 01)
@src/omni_agents/llm/response_parser.py (extract_json from Plan 01 -- fallback if SDK parsing fails)
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add generate_structured() to BaseLLM, GeminiAdapter, and OpenAIAdapter</name>
  <files>
    src/omni_agents/llm/base.py
    src/omni_agents/llm/gemini.py
    src/omni_agents/llm/openai_adapter.py
  </files>
  <action>
**1a. Add abstract method to `src/omni_agents/llm/base.py`:**

Add a new abstract method to `BaseLLM` below the existing `generate()` method. Use Python 3.12+ generic syntax (PEP 695):

```python
from pydantic import BaseModel  # already imported

@abstractmethod
async def generate_structured[T: BaseModel](
    self,
    system_prompt: str,
    user_prompt: str,
    response_model: type[T],
) -> T:
    """Generate a structured response conforming to a Pydantic model.

    Uses the provider's native structured output API to constrain
    the LLM response to match the given Pydantic schema.

    Args:
        system_prompt: System instruction for the LLM.
        user_prompt: User message (e.g., protocol document text).
        response_model: Pydantic model class the response must conform to.

    Returns:
        An instance of response_model populated by the LLM.

    Raises:
        LLMError: If the API call or response parsing fails.
    """
```

NOTE: If Python 3.12 generic syntax causes issues with the ABC metaclass, use `TypeVar` approach instead:
```python
from typing import TypeVar
_T = TypeVar("_T", bound=BaseModel)

@abstractmethod
async def generate_structured(
    self, system_prompt: str, user_prompt: str, response_model: type[_T]
) -> _T: ...
```

**1b. Implement in `src/omni_agents/llm/gemini.py`:**

Add `generate_structured()` method to `GeminiAdapter`:

```python
async def generate_structured[T: BaseModel](
    self,
    system_prompt: str,
    user_prompt: str,
    response_model: type[T],
) -> T:
    """Generate structured output using Gemini's response_schema.

    Uses response_mime_type='application/json' with response_schema
    set to the Pydantic model. The SDK handles schema conversion
    and response parsing via response.parsed.
    """
    try:
        response = await self.client.aio.models.generate_content(
            model=self.model,
            contents=user_prompt,
            config=types.GenerateContentConfig(
                system_instruction=system_prompt,
                temperature=self.temperature,
                response_mime_type="application/json",
                response_schema=response_model,
            ),
        )
    except Exception as exc:
        raise LLMError(
            provider="gemini",
            message=f"Structured generation failed: {exc}",
            original_error=exc,
        ) from exc

    # Try SDK's built-in parsing first
    if hasattr(response, "parsed") and response.parsed is not None:
        return response.parsed

    # Fallback: parse JSON from response text manually
    import json
    from omni_agents.llm.response_parser import extract_json

    raw_text = response.text or ""
    data = extract_json(raw_text)
    if data is None:
        raise LLMError(
            provider="gemini",
            message=f"Could not parse structured output from response: {raw_text[:200]}",
        )
    return response_model.model_validate(data)
```

**1c. Implement in `src/omni_agents/llm/openai_adapter.py`:**

Add `generate_structured()` method to `OpenAIAdapter`:

```python
async def generate_structured[T: BaseModel](
    self,
    system_prompt: str,
    user_prompt: str,
    response_model: type[T],
) -> T:
    """Generate structured output using OpenAI's beta parse API.

    Uses client.beta.chat.completions.parse() with response_format
    set to the Pydantic model. The SDK handles schema enforcement
    and response deserialization.
    """
    try:
        completion = await self.client.beta.chat.completions.parse(
            model=self.model,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt},
            ],
            temperature=self.temperature,
            response_format=response_model,
        )
    except APIError as exc:
        raise LLMError(
            provider="openai",
            message=f"Structured generation failed: {exc}",
            original_error=exc,
        ) from exc

    parsed = completion.choices[0].message.parsed
    if parsed is None:
        # Fallback: try manual JSON parsing from content
        import json
        from omni_agents.llm.response_parser import extract_json

        raw_text = completion.choices[0].message.content or ""
        data = extract_json(raw_text)
        if data is None:
            raise LLMError(
                provider="openai",
                message="Structured output parsing returned None",
            )
        return response_model.model_validate(data)

    return parsed
```

IMPORTANT: Both implementations include a manual JSON fallback using extract_json() from Plan 01. This guards against SDK parsing failures while leveraging native structured output when it works.
  </action>
  <verify>
Run `python -c "from omni_agents.llm.base import BaseLLM; print([m for m in dir(BaseLLM) if 'structured' in m])"` -- shows generate_structured.
Run `python -c "from omni_agents.llm.gemini import GeminiAdapter; print('OK')"` -- import succeeds (no syntax errors).
Run `python -c "from omni_agents.llm.openai_adapter import OpenAIAdapter; print('OK')"` -- import succeeds.
Run `pytest tests/ -v` -- all existing tests still pass (no regressions from adding the new abstract method -- existing tests mock BaseLLM or use concrete adapters).
  </verify>
  <done>
BaseLLM has generate_structured() abstract method. GeminiAdapter uses response_schema + response_mime_type. OpenAIAdapter uses beta.chat.completions.parse(). Both include extract_json fallback. All existing tests pass.
  </done>
</task>

<task type="auto">
  <name>Task 2: Create ProtocolParserAgent class and protocol_parser.j2 prompt template</name>
  <files>
    src/omni_agents/agents/protocol_parser.py
    src/omni_agents/templates/prompts/protocol_parser.j2
  </files>
  <action>
**2a. Create `src/omni_agents/agents/protocol_parser.py`:**

This is a STANDALONE class -- NOT a BaseAgent subclass (see ARCHITECTURE.md: "BaseAgent is designed for the generate-R-code-then-execute-in-Docker pattern").

```python
"""Protocol parser agent: extracts trial config from .docx documents.

NOT a BaseAgent subclass -- this agent produces structured data (TrialConfig),
not R code. It runs in-process (no Docker) and is invoked before the pipeline.
"""
from pathlib import Path

from jinja2 import Template

from omni_agents.agents.docx_reader import extract_protocol_text
from omni_agents.config import (
    ExtractionResult,
    ProtocolExtraction,
    TrialConfig,
    merge_extraction,
)
from omni_agents.llm.base import BaseLLM


class ProtocolParserAgent:
    """Parses a clinical trial protocol document into a TrialConfig.

    Workflow:
    1. Read .docx with python-docx (paragraphs + tables)
    2. Load system prompt from protocol_parser.j2
    3. Call LLM with structured output (ProtocolExtraction schema)
    4. Merge extraction with TrialConfig defaults
    5. Return ExtractionResult with field tracking

    Args:
        llm: Any BaseLLM adapter (Gemini recommended for single-shot extraction).
        prompt_dir: Directory containing prompt templates.
    """

    TEMPLATE_NAME = "protocol_parser.j2"

    def __init__(self, llm: BaseLLM, prompt_dir: Path) -> None:
        self.llm = llm
        self.prompt_dir = prompt_dir

    async def parse(
        self,
        protocol_path: Path,
        defaults: TrialConfig | None = None,
    ) -> ExtractionResult:
        """Extract trial parameters from a .docx protocol document.

        Args:
            protocol_path: Path to the .docx protocol file.
            defaults: Optional custom TrialConfig defaults. Uses
                TrialConfig() if not provided.

        Returns:
            ExtractionResult with merged config and field tracking.

        Raises:
            FileNotFoundError: If protocol_path does not exist.
            LLMError: If the LLM call fails.
            ValidationError: If extracted values fail Pydantic validation.
        """
        # 1. Extract document text
        document_text = extract_protocol_text(protocol_path)

        # 2. Load and render system prompt
        template_path = self.prompt_dir / self.TEMPLATE_NAME
        template_text = template_path.read_text()
        template = Template(template_text)

        # Pass TrialConfig field info to template for schema description
        field_info = self._build_field_info()
        system_prompt = template.render(fields=field_info)

        # 3. Call LLM with structured output
        extraction = await self.llm.generate_structured(
            system_prompt=system_prompt,
            user_prompt=document_text,
            response_model=ProtocolExtraction,
        )

        # 4. Merge with defaults
        return merge_extraction(extraction, defaults)

    @staticmethod
    def _build_field_info() -> list[dict[str, str]]:
        """Build field descriptions for the prompt template.

        Returns a list of dicts with name, type, description, and range
        for each TrialConfig field the LLM should extract.
        """
        return [
            {
                "name": "n_subjects",
                "type": "integer",
                "description": "Total number of subjects enrolled in the trial",
                "synonyms": "sample size, enrolled, participants, subjects, N",
                "range": "10 to 10000",
            },
            {
                "name": "randomization_ratio",
                "type": "string",
                "description": "Treatment-to-placebo randomization ratio",
                "synonyms": "randomization, allocation ratio",
                "range": "format like '2:1' or '1:1'",
            },
            {
                "name": "visits",
                "type": "integer",
                "description": "Number of study visits (including baseline)",
                "synonyms": "visits, assessments, measurement timepoints",
                "range": "2 to 100",
            },
            {
                "name": "endpoint",
                "type": "string",
                "description": "Primary efficacy endpoint abbreviation",
                "synonyms": "primary endpoint, primary outcome, efficacy measure",
                "range": "short abbreviation like 'SBP', 'DBP', 'HbA1c'",
            },
            {
                "name": "treatment_sbp_mean",
                "type": "float",
                "description": "Expected mean SBP in the treatment arm at end of study",
                "synonyms": "target SBP, treatment arm mean, active treatment blood pressure",
                "range": "50.0 to 250.0 mmHg",
            },
            {
                "name": "treatment_sbp_sd",
                "type": "float",
                "description": "Standard deviation of SBP in the treatment arm",
                "synonyms": "treatment SD, treatment variability",
                "range": "1.0 to 50.0 mmHg",
            },
            {
                "name": "placebo_sbp_mean",
                "type": "float",
                "description": "Expected mean SBP in the placebo arm at end of study",
                "synonyms": "placebo mean, control arm blood pressure",
                "range": "50.0 to 250.0 mmHg",
            },
            {
                "name": "placebo_sbp_sd",
                "type": "float",
                "description": "Standard deviation of SBP in the placebo arm",
                "synonyms": "placebo SD, control variability",
                "range": "1.0 to 50.0 mmHg",
            },
            {
                "name": "baseline_sbp_mean",
                "type": "float",
                "description": "Mean baseline SBP across all subjects at enrollment",
                "synonyms": "baseline blood pressure, enrollment SBP, screening SBP",
                "range": "50.0 to 250.0 mmHg",
            },
            {
                "name": "baseline_sbp_sd",
                "type": "float",
                "description": "Standard deviation of baseline SBP",
                "synonyms": "baseline variability, baseline SD",
                "range": "1.0 to 50.0 mmHg",
            },
            {
                "name": "age_mean",
                "type": "float",
                "description": "Mean age of study population",
                "synonyms": "average age, mean participant age",
                "range": "18.0 to 100.0 years",
            },
            {
                "name": "age_sd",
                "type": "float",
                "description": "Standard deviation of age in study population",
                "synonyms": "age variability, age SD",
                "range": "1.0 to 30.0 years",
            },
            {
                "name": "missing_rate",
                "type": "float",
                "description": "Expected rate of missing data (as decimal fraction 0.0-1.0)",
                "synonyms": "missing data rate, data completeness gap",
                "range": "0.0 to 1.0 (MUST be decimal fraction, NOT percentage. If protocol says '3%', extract 0.03)",
            },
            {
                "name": "dropout_rate",
                "type": "float",
                "description": "Expected dropout/discontinuation rate (as decimal fraction 0.0-1.0)",
                "synonyms": "dropout, discontinuation rate, withdrawal rate, attrition",
                "range": "0.0 to 1.0 (MUST be decimal fraction, NOT percentage. If protocol says '10%', extract 0.10)",
            },
        ]
```

**2b. Create `src/omni_agents/templates/prompts/protocol_parser.j2`:**

This is the critical prompt template that addresses PITFALL-01 (silent misextraction) and PITFALL-08 (prompt engineering). It must:
- Enumerate all fields with descriptions, synonyms, and ranges
- Instruct the LLM to return null for fields not found (not to guess)
- Warn about common misextraction errors (percentage vs decimal, inverted ratios)

```jinja2
You are a clinical trial protocol data extractor. Your task is to read a clinical trial protocol document and extract specific numerical and categorical parameters into a structured JSON format.

## EXTRACTION RULES

1. Extract ONLY values explicitly stated in the document. If a parameter is not mentioned or is ambiguous, return null for that field.
2. DO NOT guess, interpolate, or hallucinate values. A null field is better than a wrong field.
3. Pay attention to units and scales:
   - Rates (dropout_rate, missing_rate) must be DECIMAL FRACTIONS (0.0-1.0), NOT percentages. If the document says "10%", extract 0.10.
   - Blood pressure values are in mmHg.
   - Age is in years.
4. Distinguish between baseline, treatment, and placebo values carefully. These often appear in different sections or table rows.
5. The randomization_ratio must be in "X:Y" format (e.g., "2:1" means 2 treatment : 1 placebo).

## FIELDS TO EXTRACT

{% for field in fields %}
### {{ field.name }} ({{ field.type }})
- **Description:** {{ field.description }}
- **Look for:** {{ field.synonyms }}
- **Valid range:** {{ field.range }}

{% endfor %}

## OUTPUT FORMAT

Return a JSON object with exactly these field names. Use null for any field you cannot confidently extract from the document.

Example output:
```json
{
  "n_subjects": 300,
  "randomization_ratio": "2:1",
  "visits": 26,
  "endpoint": "SBP",
  "treatment_sbp_mean": 120.0,
  "treatment_sbp_sd": null,
  "placebo_sbp_mean": 140.0,
  "placebo_sbp_sd": null,
  "baseline_sbp_mean": 150.0,
  "baseline_sbp_sd": 10.0,
  "age_mean": 55.0,
  "age_sd": 10.0,
  "missing_rate": 0.03,
  "dropout_rate": 0.10
}
```

IMPORTANT: Return null (not a default value) for any field where the document does not provide a clear, unambiguous value. The system will handle defaults separately and flag them for user review.
```

IMPORTANT NOTES on the template:
- The `fields` variable is passed from `_build_field_info()` in the agent
- The template does NOT include `seed` because seed is a simulation parameter, not a protocol parameter
- The null instruction is critical for PITFALL-04 (defaults fill gaps silently)
- The percentage-to-decimal warning is critical for PITFALL-01 (unit confusion)
  </action>
  <verify>
Run `python -c "from omni_agents.agents.protocol_parser import ProtocolParserAgent; print('OK')"` -- import works.
Run `python -c "
from pathlib import Path
from jinja2 import Template
t = Path('src/omni_agents/templates/prompts/protocol_parser.j2').read_text()
tmpl = Template(t)
from omni_agents.agents.protocol_parser import ProtocolParserAgent
fields = ProtocolParserAgent._build_field_info()
rendered = tmpl.render(fields=fields)
assert 'n_subjects' in rendered
assert 'dropout_rate' in rendered
assert 'DECIMAL FRACTIONS' in rendered
assert len(fields) == 14
print(f'Template renders OK, {len(fields)} fields, {len(rendered)} chars')
"` -- template renders with all 14 fields.
Run `pytest tests/ -v` -- all existing tests still pass.
  </verify>
  <done>
BaseLLM/GeminiAdapter/OpenAIAdapter have generate_structured() method using native SDK structured output APIs. ProtocolParserAgent is a standalone class (not BaseAgent subclass) that reads .docx, calls LLM with ProtocolExtraction schema, merges with defaults. Prompt template enumerates all 14 fields with descriptions, ranges, synonyms, and explicit null-for-missing instruction. All existing tests pass.
  </done>
</task>

</tasks>

<verification>
1. `python -c "from omni_agents.agents.protocol_parser import ProtocolParserAgent"` -- import works
2. Template renders with all 14 TrialConfig fields (excluding seed)
3. `pytest tests/ -v` -- all existing tests pass (no regressions from BaseLLM changes)
4. GeminiAdapter and OpenAIAdapter both implement generate_structured with fallback to extract_json
5. ProtocolParserAgent.parse() chains: extract_protocol_text -> generate_structured -> merge_extraction
</verification>

<success_criteria>
- generate_structured() is abstract on BaseLLM and implemented on both adapters
- ProtocolParserAgent is standalone (not BaseAgent subclass), uses BaseLLM.generate_structured()
- Prompt template has extraction rules for all 14 fields with ranges and synonyms
- Template warns about decimal fraction vs percentage (PITFALL-01)
- Template instructs null for missing fields (PITFALL-04)
- All existing tests pass
</success_criteria>

<output>
After completion, create `.planning/phases/07-protocol-parser/07-02-SUMMARY.md`
</output>
