---
phase: 01-symmetric-double-programming
plan: 02
type: execute
wave: 2
depends_on: ["01-01"]
files_modified:
  - src/omni_agents/pipeline/stage_comparator.py
  - tests/test_pipeline/test_stage_comparator.py
autonomous: true

must_haves:
  truths:
    - "SDTM comparison detects row count, column, subject ID, and distribution mismatches between tracks"
    - "ADaM comparison detects n_rows, n_events, n_censored, PARAMCD, and column mismatches"
    - "Stats comparison uses tolerances matching ConsensusJudge (logrank_p abs 1e-3, cox_hr rel 0.1%, km_median abs 0.5)"
    - "compare_all_stages returns StageComparisonResult with per-stage results"
    - "When both tracks produce identical outputs, all stages report matches=True"
  artifacts:
    - path: "src/omni_agents/pipeline/stage_comparator.py"
      provides: "StageComparator class with compare_sdtm, compare_adam, compare_stats, compare_all_stages"
      exports: ["StageComparator"]
    - path: "tests/test_pipeline/test_stage_comparator.py"
      provides: "Unit tests for all three stage comparators plus the aggregator"
      min_lines: 80
  key_links:
    - from: "src/omni_agents/pipeline/stage_comparator.py"
      to: "src/omni_agents/models/resolution.py"
      via: "returns StageComparison and StageComparisonResult"
      pattern: "from omni_agents.models.resolution import.*StageComparison"
    - from: "src/omni_agents/pipeline/stage_comparator.py"
      to: "src/omni_agents/pipeline/consensus.py"
      via: "reuses tolerance values from ConsensusJudge.TOLERANCES"
      pattern: "TOLERANCES|tolerance|threshold"
---

<objective>
Build the StageComparator class that compares Track A and Track B outputs at each pipeline stage (SDTM, ADaM, Stats).

Purpose: Stage-by-stage comparison is the core of the symmetric double programming validation. It replaces the current single-point ConsensusJudge comparison with granular per-stage checks, enabling precise diagnosis of WHERE tracks diverge.

Output: New `pipeline/stage_comparator.py` with comparison logic for all three stages + comprehensive unit tests.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/01-symmetric-double-programming/01-RESEARCH.md

# Prior plan artifacts needed:
@src/omni_agents/models/resolution.py

# Existing patterns to follow:
@src/omni_agents/pipeline/consensus.py
@src/omni_agents/pipeline/schema_validator.py
@src/omni_agents/models/consensus.py
@src/omni_agents/models/schemas.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create StageComparator class</name>
  <files>src/omni_agents/pipeline/stage_comparator.py</files>
  <action>
Create `src/omni_agents/pipeline/stage_comparator.py` with a `StageComparator` class. Follow the style of `SchemaValidator` (classmethods, CSV reading) and `ConsensusJudge` (tolerance specs).

Module docstring: "Stage-by-stage comparison of Track A and Track B pipeline outputs."

**Class: StageComparator**

All methods are `@classmethod`. No instance state.

**Helper `_read_csv(path: Path) -> list[dict[str, str]]`**: Same pattern as SchemaValidator._read_csv. Use csv.DictReader.

**Method `compare_sdtm(track_a_dir: Path, track_b_dir: Path, expected_subjects: int) -> StageComparison`**:

Compare DM.csv and VS.csv between tracks. EXACT match tolerance (zero) per research spec.

Checks (in order):
1. **DM row count**: len(dm_a) vs len(dm_b). Issue if different.
2. **VS row count**: len(vs_a) vs len(vs_b). Issue if different.
3. **DM column sets**: set(dm_a[0].keys()) vs set(dm_b[0].keys()). Issue if different.
4. **VS column sets**: set(vs_a[0].keys()) vs set(vs_b[0].keys()). Issue if different.
5. **Subject ID set equality**: {row["USUBJID"] for row in dm_a} vs dm_b. Issue if different (report count of only-in-A and only-in-B).
6. **ARM distribution**: Counter of ARM values from DM. Issue if different.
7. **SEX distribution**: Counter of SEX values from DM. Issue if different.
8. **RACE distribution**: Counter of RACE values from DM. Issue if different.

Build `track_a_summary` and `track_b_summary` dicts with `{"dm_rows": N, "vs_rows": N, "subjects": N}`.

Return `StageComparison(stage="sdtm", matches=len(issues)==0, issues=issues, track_a_summary=..., track_b_summary=...)`.

Import `StageComparison` and `StageComparisonResult` from `omni_agents.models.resolution`.
Import `Counter` from `collections`.

**Method `compare_adam(track_a_dir: Path, track_b_dir: Path, expected_subjects: int) -> StageComparison`**:

Compare ADTTE_summary.json between tracks. EXACT match tolerance.

Read both track's `ADTTE_summary.json` via json.loads. Parse into dicts.

Checks:
1. **n_rows**: Exact match between tracks.
2. **n_events**: Exact match.
3. **n_censored**: Exact match.
4. **PARAMCD**: Exact string match.
5. **Column sets**: Compare "columns" arrays from both summaries.

Build summaries with `{"n_rows": N, "n_events": N, "n_censored": N, "paramcd": str}`.

**Method `compare_stats(track_a_dir: Path, track_b_dir: Path) -> StageComparison`**:

Compare results.json between tracks using ConsensusJudge-compatible tolerances.

Read both track's `results.json`. Both now have the SAME format (table2, table3, metadata keys) since both tracks run the full pipeline.

Tolerance spec (reuse values from ConsensusJudge.TOLERANCES, do NOT import from ConsensusJudge -- define locally to avoid coupling):
```python
STATS_TOLERANCES = {
    "n_subjects": {"type": "exact"},
    "n_events": {"type": "exact"},
    "n_censored": {"type": "exact"},
    "logrank_p": {"type": "absolute", "threshold": 1e-3},
    "cox_hr": {"type": "relative", "threshold": 0.001},
    "km_median_treatment": {"type": "absolute", "threshold": 0.5},
    "km_median_placebo": {"type": "absolute", "threshold": 0.5},
}
```

Checks (extract values from results.json):
1. Structural: metadata.n_subjects, metadata.n_events, metadata.n_censored -- exact match.
2. Statistical: table2.logrank_p, table3.cox_hr, table2.km_median_treatment, table2.km_median_placebo -- use tolerance.

For relative tolerance, use `math.isclose(a, b, rel_tol=threshold, abs_tol=0)`.
For absolute tolerance, use `abs(a - b) <= threshold`.

Build summaries with all compared metric values.

**Method `compare_all_stages(track_a_result: TrackResult, track_b_result: TrackResult, expected_subjects: int) -> StageComparisonResult`**:

Convenience method that calls compare_sdtm, compare_adam, compare_stats in sequence and returns a `StageComparisonResult` with all three comparisons.

Import `TrackResult` from `omni_agents.models.resolution`.
  </action>
  <verify>
Run: `cd /Users/sanmaysarada/omni-ai-agents && python -c "from omni_agents.pipeline.stage_comparator import StageComparator; print('StageComparator importable'); print(dir(StageComparator))"`
  </verify>
  <done>StageComparator class exists with compare_sdtm, compare_adam, compare_stats, compare_all_stages classmethods. All return StageComparison or StageComparisonResult models.</done>
</task>

<task type="auto">
  <name>Task 2: Write StageComparator unit tests</name>
  <files>tests/test_pipeline/test_stage_comparator.py</files>
  <action>
Create `tests/test_pipeline/test_stage_comparator.py` with comprehensive tests. Follow the style of existing `test_script_cache.py` (pytest class-based, tmp_path fixture).

**Fixture helpers** (module-level functions or fixtures):

`_write_csv(path: Path, rows: list[dict])` -- write a list of dicts to a CSV file using csv.DictWriter.

`_write_json(path: Path, data: dict)` -- write dict as JSON to path.

`_make_dm_rows(n: int, arms: dict | None = None) -> list[dict]`:
Generate n DM rows with USUBJID=f"SUBJ-{i:03d}", ARM cycling through Treatment/Placebo at 2:1 ratio, SEX="M"/"F" alternating, RACE="WHITE", AGE="55", STUDYID="SBP-001", SITEID="SITE-01".

`_make_vs_rows(dm_rows: list[dict], visits: int = 26) -> list[dict]`:
For each DM row, generate `visits` VS rows with USUBJID matching, VISIT=f"VISIT {v}", VSTESTCD="SBP", VSSTRESN="120".

`_make_adam_summary(n_rows: int, n_events: int) -> dict`:
Return `{"n_rows": n_rows, "n_events": n_events, "n_censored": n_rows - n_events, "paramcd": "TTESB120", "columns": ["USUBJID", "PARAMCD", "AVAL", "CNSR", "ARM", "AGE", "SEX"]}`.

`_make_stats_results(p: float = 0.032, hr: float = 0.75, km_treat: float = 18.5, km_plac: float = 14.2, n_sub: int = 300, n_events: int = 180, n_cens: int = 120) -> dict`:
Return `{"metadata": {"n_subjects": n_sub, "n_events": n_events, "n_censored": n_cens}, "table2": {"logrank_p": p, "km_median_treatment": km_treat, "km_median_placebo": km_plac}, "table3": {"cox_hr": hr}}`.

**Test class: TestCompareSDTM**:

1. `test_sdtm_identical_tracks_match(tmp_path)` -- create identical DM.csv and VS.csv in track_a/sdtm and track_b/sdtm. Assert comparison.matches is True and issues is empty.

2. `test_sdtm_different_row_count(tmp_path)` -- Track A has 300 DM rows, Track B has 298. Assert matches=False, "DM row count" in issues.

3. `test_sdtm_different_subject_ids(tmp_path)` -- Track A and B have same count but 2 different USUBJIDs. Assert matches=False, "Subject ID" in issues.

4. `test_sdtm_different_arm_distribution(tmp_path)` -- Same subjects but different ARM assignments. Assert matches=False, "ARM distribution" in issues.

**Test class: TestCompareADaM**:

5. `test_adam_identical_match(tmp_path)` -- identical ADTTE_summary.json. Assert matches=True.

6. `test_adam_different_events(tmp_path)` -- Track A n_events=180, Track B n_events=175. Assert matches=False.

**Test class: TestCompareStats**:

7. `test_stats_identical_match(tmp_path)` -- identical results.json. Assert matches=True.

8. `test_stats_within_tolerance(tmp_path)` -- logrank_p differs by 0.0005 (within 1e-3). Assert matches=True.

9. `test_stats_outside_tolerance(tmp_path)` -- logrank_p differs by 0.005 (outside 1e-3). Assert matches=False.

10. `test_stats_exact_metric_mismatch(tmp_path)` -- n_subjects differs. Assert matches=False.

**Test class: TestCompareAllStages**:

11. `test_all_stages_pass(tmp_path)` -- set up all three stages as matching. Assert has_disagreement is False.

12. `test_first_disagreement_found(tmp_path)` -- SDTM matches, ADaM disagrees. Assert first_disagreement.stage == "adam".
  </action>
  <verify>
Run: `cd /Users/sanmaysarada/omni-ai-agents && python -m pytest tests/test_pipeline/test_stage_comparator.py -v`
  </verify>
  <done>All 12 StageComparator tests pass. Tests cover identical match, row count mismatch, subject ID mismatch, distribution mismatch, tolerance boundary (within and outside), structural mismatch, and the compare_all_stages aggregator with has_disagreement and first_disagreement properties.</done>
</task>

</tasks>

<verification>
1. `python -m pytest tests/test_pipeline/test_stage_comparator.py -v` -- all 12 tests pass
2. `python -m pytest tests/ -v` -- no regressions
3. `python -c "from omni_agents.pipeline.stage_comparator import StageComparator; print('OK')"` -- importable
</verification>

<success_criteria>
- StageComparator.compare_sdtm checks row counts, columns, subject IDs, ARM/SEX/RACE distributions
- StageComparator.compare_adam checks n_rows, n_events, n_censored, PARAMCD, columns
- StageComparator.compare_stats uses tolerances matching ConsensusJudge (logrank_p abs 1e-3, cox_hr rel 0.1%, km_median abs 0.5)
- StageComparator.compare_all_stages returns StageComparisonResult with has_disagreement property
- 12 unit tests pass covering match, mismatch, tolerance boundary cases
</success_criteria>

<output>
After completion, create `.planning/phases/01-symmetric-double-programming/01-02-SUMMARY.md`
</output>
