---
phase: 01-symmetric-double-programming
plan: 04
type: execute
wave: 3
depends_on: ["01-02", "01-03"]
files_modified:
  - src/omni_agents/pipeline/resolution.py
  - src/omni_agents/pipeline/orchestrator.py
  - tests/test_pipeline/test_resolution.py
autonomous: true

must_haves:
  truths:
    - "When tracks disagree at a stage, the resolution loop diagnoses which track erred using deterministic rules"
    - "Resolution provides structured hints (discrepancies + validation failures + suggested checks) to the failing track"
    - "Failed track retries from the disagreeing stage only, not full restart"
    - "Resolution is bounded at max 2 iterations to prevent infinite loops"
    - "When tracks agree after resolution, pipeline proceeds with PASS verdict"
    - "When resolution exhausts iterations, pipeline HALTs or uses best-validated track with WARNING"
    - "Stage-by-stage comparison replaces ConsensusJudge in the orchestrator run() method"
    - "Resolution metadata (iterations, stage, resolved status) is saved to consensus/resolution_log.json"
  artifacts:
    - path: "src/omni_agents/pipeline/resolution.py"
      provides: "ResolutionLoop class with resolve, _diagnose, _generate_hint, _rerun_from_stage methods"
      exports: ["ResolutionLoop"]
    - path: "src/omni_agents/pipeline/orchestrator.py"
      provides: "Orchestrator with stage comparison and resolution loop integrated into run()"
      contains: "StageComparator"
    - path: "tests/test_pipeline/test_resolution.py"
      provides: "Unit tests for ResolutionLoop diagnosis and hint generation"
      min_lines: 60
  key_links:
    - from: "src/omni_agents/pipeline/resolution.py"
      to: "src/omni_agents/pipeline/stage_comparator.py"
      via: "calls StageComparator for re-comparison after retry"
      pattern: "StageComparator"
    - from: "src/omni_agents/pipeline/resolution.py"
      to: "src/omni_agents/models/resolution.py"
      via: "uses ResolutionHint, ResolutionResult, StageComparison"
      pattern: "from omni_agents.models.resolution import"
    - from: "src/omni_agents/pipeline/orchestrator.py"
      to: "src/omni_agents/pipeline/stage_comparator.py"
      via: "calls StageComparator.compare_all_stages after parallel tracks complete"
      pattern: "StageComparator.compare_all_stages"
    - from: "src/omni_agents/pipeline/orchestrator.py"
      to: "src/omni_agents/pipeline/resolution.py"
      via: "calls ResolutionLoop.resolve when disagreement detected"
      pattern: "ResolutionLoop.*resolve"
---

<objective>
Build the adversarial resolution loop and integrate stage-by-stage comparison + resolution into the orchestrator's run() method.

Purpose: This is the capstone plan that makes the symmetric architecture operational. After this plan, the pipeline: (1) runs both tracks in parallel, (2) compares outputs at every stage, (3) diagnoses which track erred when they disagree, (4) retries the failing track with targeted hints, and (5) produces a final verdict with resolution metadata.

Output: New `pipeline/resolution.py` with ResolutionLoop class; updated orchestrator with stage comparison + resolution replacing ConsensusJudge; unit tests for resolution logic.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/01-symmetric-double-programming/01-RESEARCH.md

# Prior plan artifacts (all three):
@src/omni_agents/models/resolution.py
@src/omni_agents/pipeline/stage_comparator.py
@src/omni_agents/pipeline/orchestrator.py

# Referenced:
@src/omni_agents/pipeline/schema_validator.py
@src/omni_agents/pipeline/consensus.py
@src/omni_agents/pipeline/retry.py
@src/omni_agents/display/callbacks.py
@src/omni_agents/config.py
@src/omni_agents/agents/base.py
@src/omni_agents/agents/sdtm.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create ResolutionLoop class</name>
  <files>
    src/omni_agents/pipeline/resolution.py
    tests/test_pipeline/test_resolution.py
  </files>
  <action>
Create `src/omni_agents/pipeline/resolution.py` with the `ResolutionLoop` class.

Module docstring: "Adversarial resolution loop for semantic disagreements between symmetric tracks."

**Imports:**
```python
from __future__ import annotations

import json
from pathlib import Path
from typing import TYPE_CHECKING

from loguru import logger

from omni_agents.models.resolution import (
    ResolutionHint,
    ResolutionResult,
    StageComparison,
    TrackResult,
)
from omni_agents.pipeline.stage_comparator import StageComparator

if TYPE_CHECKING:
    from omni_agents.pipeline.orchestrator import PipelineOrchestrator
```

**Class: ResolutionLoop**

```python
class ResolutionLoop:
    """Orchestrate adversarial resolution when symmetric tracks disagree.

    Resolution protocol:
    1. DETECT: StageComparator finds disagreement at stage S
    2. DIAGNOSE: Deterministic validation rules check which track erred
    3. HINT: Generate structured hint for failing track
    4. RETRY: Re-run failing track from stage S with hint
    5. RE-COMPARE: Compare new output with other track
    6. TERMINATE: After max iterations, pick best track or HALT
    """

    def __init__(self, max_iterations: int = 2) -> None:
        self.max_iterations = max_iterations
```

**Method: `async def resolve(...) -> ResolutionResult`**

Parameters:
- `disagreement: StageComparison` -- the stage that disagreed
- `track_a_result: TrackResult` -- current Track A result
- `track_b_result: TrackResult` -- current Track B result
- `orchestrator: PipelineOrchestrator` -- reference to orchestrator for re-running tracks
- `expected_subjects: int` -- for schema validation on re-comparison

Logic:
```
resolution_log = []
current_disagreement = disagreement

for iteration in range(1, self.max_iterations + 1):
    # 1. Diagnose which track is more likely wrong
    failing_track = self._diagnose(current_disagreement, track_a_result, track_b_result)
    resolution_log.append(f"Iteration {iteration}: diagnosed {failing_track} as likely failing")

    # 2. Generate hint
    hint = self._generate_hint(current_disagreement, failing_track)
    resolution_log.append(f"Hint generated for {failing_track}: {len(hint.discrepancies)} discrepancies")

    # 3. Re-run failing track from disagreeing stage
    if failing_track == "track_a":
        new_result = await self._rerun_from_stage(
            track_a_result, current_disagreement.stage, hint, orchestrator
        )
        track_a_result = new_result
    else:
        new_result = await self._rerun_from_stage(
            track_b_result, current_disagreement.stage, hint, orchestrator
        )
        track_b_result = new_result

    # 4. Re-compare the stage
    new_comparison = self._recompare_stage(
        current_disagreement.stage, track_a_result, track_b_result, expected_subjects
    )

    if new_comparison.matches:
        resolution_log.append(f"Iteration {iteration}: tracks now agree at {current_disagreement.stage}")
        return ResolutionResult(
            resolved=True, iterations=iteration, winning_track=None,
            stage=current_disagreement.stage, resolution_log=resolution_log,
        )

    current_disagreement = new_comparison
    resolution_log.append(f"Iteration {iteration}: still disagreeing")

# Max iterations reached
best_track = self._pick_best_track(current_disagreement)
resolution_log.append(f"Max iterations reached. Best track: {best_track or 'NONE (HALT)'}")
return ResolutionResult(
    resolved=False, iterations=self.max_iterations, winning_track=best_track,
    stage=current_disagreement.stage, resolution_log=resolution_log,
)
```

**Method: `_diagnose(disagreement, track_a_result, track_b_result) -> str`**

Returns "track_a" or "track_b" -- which track is more likely wrong.

Deterministic diagnosis (priority order from research):
1. **Schema validation score:** For the disagreeing stage, check if one track's output has more issues in the comparison. The track with MORE issues in StageComparison is more likely wrong.
2. **Consistency with expected values:** If one track's summary shows unexpected values (e.g., n_rows != expected_subjects for SDTM), that track is wrong.
3. **Default:** If ambiguous, default to "track_b" (GPT-4o is the newer, less tested track in this pipeline). This is a heuristic tiebreaker.

For simplicity in V1: parse the issues list from the StageComparison. Check if issues mention specific tracks. If issues mention counts where one track has a value further from the expected, that track is failing. If truly ambiguous, default to "track_b".

Implementation approach:
```python
def _diagnose(self, disagreement: StageComparison, track_a_result: TrackResult, track_b_result: TrackResult) -> str:
    """Diagnose which track is more likely wrong based on deterministic rules."""
    # Parse summaries for expected value deviation
    a_summary = disagreement.track_a_summary
    b_summary = disagreement.track_b_summary

    # Heuristic: if one track has fewer rows/subjects than expected, it dropped data
    for key in ("dm_rows", "n_rows", "subjects"):
        a_val = a_summary.get(key)
        b_val = b_summary.get(key)
        if a_val is not None and b_val is not None and a_val != b_val:
            # The track with fewer items likely dropped data
            if a_val < b_val:
                return "track_a"
            return "track_b"

    # Default: assume track_b (secondary LLM) is more likely to have issues
    logger.info("Diagnosis ambiguous, defaulting to track_b as failing")
    return "track_b"
```

**Method: `_generate_hint(disagreement, failing_track) -> ResolutionHint`**

Build a ResolutionHint from the StageComparison:
- `stage`: from disagreement.stage
- `discrepancies`: from disagreement.issues
- `validation_failures`: empty list for V1 (future: run SchemaValidator and collect failures)
- `suggested_checks`: derive from stage type:
  - sdtm: ["Check deduplication logic", "Verify all subjects from raw data are included", "Check CDISC controlled terminology mapping"]
  - adam: ["Check event definition (SBP < 120 threshold)", "Verify all SDTM subjects flow through", "Check CNSR derivation logic"]
  - stats: ["Check Cox model specification", "Verify log-rank test parameters", "Check KM estimation method"]

**Method: `async def _rerun_from_stage(track_result, stage, hint, orchestrator) -> TrackResult`**

Re-runs a single stage for a track with the resolution hint injected into the agent context.

Key implementation:
```python
async def _rerun_from_stage(
    self,
    track_result: TrackResult,
    stage: str,
    hint: ResolutionHint,
    orchestrator: PipelineOrchestrator,
) -> TrackResult:
    """Re-run a single stage for the failing track with a resolution hint."""
    track_id = track_result.track_id

    # Determine which LLM to use
    if track_id == "track_a":
        from omni_agents.llm.gemini import GeminiAdapter
        llm = GeminiAdapter(orchestrator.settings.llm.gemini)
    else:
        from omni_agents.llm.openai_adapter import OpenAIAdapter
        llm = OpenAIAdapter(orchestrator.settings.llm.openai)

    prompt_dir = Path(__file__).parent.parent / "templates" / "prompts"

    if stage == "sdtm":
        from omni_agents.agents.sdtm import SDTMAgent
        agent = SDTMAgent(llm=llm, prompt_dir=prompt_dir, trial_config=orchestrator.settings.trial)
        sdtm_dir = track_result.sdtm_dir
        # Build context with resolution hint
        context = {
            "input_path": "/workspace/input/SBPdata.csv",
            "output_dir": "/workspace",
            "resolution_hint": hint.to_prompt_text(),
        }
        # Find raw_dir (parent of track dir, then "raw")
        raw_dir = track_result.sdtm_dir.parent.parent / "raw"
        await orchestrator._run_agent(
            agent=agent, context=context, work_dir=sdtm_dir,
            input_volumes={str(raw_dir): "/workspace/input"},
            expected_inputs=["/workspace/input/SBPdata.csv"],
            expected_outputs=["DM.csv", "VS.csv"],
            track_id=track_id,
        )
        # Re-validate
        from omni_agents.pipeline.schema_validator import SchemaValidator
        SchemaValidator.validate_sdtm(sdtm_dir, orchestrator.settings.trial.n_subjects)
        # Return updated TrackResult (only sdtm_dir content changed)
        return track_result

    elif stage == "adam":
        from omni_agents.agents.adam import ADaMAgent
        agent = ADaMAgent(llm=llm, prompt_dir=prompt_dir, trial_config=orchestrator.settings.trial)
        adam_dir = track_result.adam_dir
        context = {
            "input_dir": "/workspace/input",
            "output_dir": "/workspace",
            "resolution_hint": hint.to_prompt_text(),
        }
        await orchestrator._run_agent(
            agent=agent, context=context, work_dir=adam_dir,
            input_volumes={str(track_result.sdtm_dir): "/workspace/input"},
            expected_inputs=["DM.csv", "VS.csv"],
            expected_outputs=["ADTTE.rds", "ADTTE_summary.json"],
            track_id=track_id,
        )
        from omni_agents.pipeline.schema_validator import SchemaValidator
        SchemaValidator.validate_adam(adam_dir, orchestrator.settings.trial.n_subjects)
        return track_result

    elif stage == "stats":
        from omni_agents.agents.stats import StatsAgent
        agent = StatsAgent(llm=llm, prompt_dir=prompt_dir, trial_config=orchestrator.settings.trial)
        stats_dir = track_result.stats_dir
        context = {
            "adam_dir": "/workspace/adam",
            "sdtm_dir": "/workspace/sdtm",
            "output_dir": "/workspace",
            "resolution_hint": hint.to_prompt_text(),
        }
        await orchestrator._run_agent(
            agent=agent, context=context, work_dir=stats_dir,
            input_volumes={
                str(track_result.adam_dir): "/workspace/adam",
                str(track_result.sdtm_dir): "/workspace/sdtm",
            },
            expected_inputs=["ADTTE.rds", "DM.csv"],
            expected_outputs=["results.json", "km_plot.png"],
            track_id=track_id,
        )
        from omni_agents.pipeline.schema_validator import SchemaValidator
        SchemaValidator.validate_stats(stats_dir)
        return track_result

    else:
        raise ValueError(f"Unknown stage: {stage}")
```

Note: The agents' `build_user_prompt` methods include `resolution_hint` from context when present (it falls through to the `context.get("resolution_hint")` -- the base agent passes all context keys). HOWEVER, the existing agents' `build_user_prompt` methods do NOT read "resolution_hint" from context. So the resolution hint needs to be injected differently.

Approach: Use `make_retry_context` pattern. The resolution hint should be passed as if it's a "previous error" for retry. The agents already handle `previous_error` in their `build_user_prompt`. The resolution hint IS essentially error feedback -- just from cross-track comparison instead of Docker stderr.

Updated approach for context injection: Instead of adding a "resolution_hint" key, pass the hint text as `previous_error` with a special prefix so the agent treats it as error feedback:
```python
context = {
    "input_path": "/workspace/input/SBPdata.csv",
    "output_dir": "/workspace",
    "previous_error": hint.to_prompt_text(),
    "attempt_number": 1,  # Resolution attempt
}
```

This leverages the existing retry mechanism -- the agent's `build_user_prompt` already handles `previous_error` in context and includes it in the prompt. The ResolutionHint.to_prompt_text() output is structured enough to serve as error feedback.

**Method: `_recompare_stage(stage, track_a_result, track_b_result, expected_subjects) -> StageComparison`**

Dispatches to the appropriate StageComparator method:
```python
def _recompare_stage(self, stage: str, track_a_result: TrackResult, track_b_result: TrackResult, expected_subjects: int) -> StageComparison:
    if stage == "sdtm":
        return StageComparator.compare_sdtm(track_a_result.sdtm_dir, track_b_result.sdtm_dir, expected_subjects)
    elif stage == "adam":
        return StageComparator.compare_adam(track_a_result.adam_dir, track_b_result.adam_dir, expected_subjects)
    elif stage == "stats":
        return StageComparator.compare_stats(track_a_result.stats_dir, track_b_result.stats_dir)
    else:
        raise ValueError(f"Unknown stage: {stage}")
```

**Method: `_pick_best_track(disagreement) -> str | None`**

If one track has clearly fewer issues (fewer items in issues list), return that track_id.
If ambiguous (equal issues), return None (triggers HALT).

For V1: return "track_a" as default winner when ambiguous (Gemini is the established track).

---

**Tests** (`tests/test_pipeline/test_resolution.py`):

Test the synchronous, deterministic parts of ResolutionLoop (diagnosis, hint generation, pick_best_track). Do NOT test the async resolve() method -- that requires mocking the full orchestrator.

```python
class TestDiagnose:
    def test_diagnose_fewer_rows_track_a(self):
        """Track with fewer rows is diagnosed as failing."""
        loop = ResolutionLoop(max_iterations=2)
        disagreement = StageComparison(
            stage="sdtm", matches=False,
            issues=["DM row count: track_a=298, track_b=300"],
            track_a_summary={"dm_rows": 298, "vs_rows": 7748},
            track_b_summary={"dm_rows": 300, "vs_rows": 7800},
        )
        # Track A has fewer rows -> Track A is likely wrong
        result = loop._diagnose(disagreement, mock_track_a, mock_track_b)
        assert result == "track_a"

    def test_diagnose_fewer_rows_track_b(self):
        """Track B with fewer rows is diagnosed as failing."""
        loop = ResolutionLoop(max_iterations=2)
        disagreement = StageComparison(
            stage="sdtm", matches=False,
            issues=["DM row count: track_a=300, track_b=295"],
            track_a_summary={"dm_rows": 300, "vs_rows": 7800},
            track_b_summary={"dm_rows": 295, "vs_rows": 7670},
        )
        result = loop._diagnose(disagreement, mock_track_a, mock_track_b)
        assert result == "track_b"

    def test_diagnose_ambiguous_defaults_track_b(self):
        """When diagnosis is ambiguous, default to track_b."""
        loop = ResolutionLoop(max_iterations=2)
        disagreement = StageComparison(
            stage="stats", matches=False,
            issues=["logrank_p differs: 0.031 vs 0.037"],
            track_a_summary={"logrank_p": 0.031},
            track_b_summary={"logrank_p": 0.037},
        )
        result = loop._diagnose(disagreement, mock_track_a, mock_track_b)
        assert result == "track_b"

class TestGenerateHint:
    def test_hint_structure(self):
        """Hint contains stage, discrepancies, and suggested checks."""
        loop = ResolutionLoop()
        disagreement = StageComparison(
            stage="sdtm", matches=False,
            issues=["DM row count: 298 vs 300", "Subject ID mismatch"],
            track_a_summary={}, track_b_summary={},
        )
        hint = loop._generate_hint(disagreement, "track_b")
        assert hint.stage == "sdtm"
        assert len(hint.discrepancies) == 2
        assert len(hint.suggested_checks) > 0
        text = hint.to_prompt_text()
        assert "RESOLUTION HINT" in text
        assert "DM row count" in text

    def test_hint_suggested_checks_per_stage(self):
        """Each stage gets stage-appropriate suggested checks."""
        loop = ResolutionLoop()
        for stage in ("sdtm", "adam", "stats"):
            disagreement = StageComparison(
                stage=stage, matches=False, issues=["some issue"],
                track_a_summary={}, track_b_summary={},
            )
            hint = loop._generate_hint(disagreement, "track_b")
            assert hint.stage == stage
            assert len(hint.suggested_checks) >= 2

class TestPickBestTrack:
    def test_pick_track_with_fewer_issues(self):
        """Track with fewer comparison issues is picked."""
        # This tests _pick_best_track behavior -- implementation detail
        pass  # Depends on implementation

    def test_ambiguous_returns_track_a(self):
        """When ambiguous, default winner is track_a."""
        loop = ResolutionLoop()
        disagreement = StageComparison(
            stage="stats", matches=False,
            issues=["some issue"],
            track_a_summary={"logrank_p": 0.031},
            track_b_summary={"logrank_p": 0.037},
        )
        result = loop._pick_best_track(disagreement)
        assert result == "track_a"
```

Use mock TrackResult objects (just need track_id, can use dummy Paths):
```python
from pathlib import Path
mock_track_a = TrackResult(track_id="track_a", sdtm_dir=Path("/tmp/a/sdtm"), adam_dir=Path("/tmp/a/adam"), stats_dir=Path("/tmp/a/stats"), results_path=Path("/tmp/a/stats/results.json"))
mock_track_b = TrackResult(track_id="track_b", sdtm_dir=Path("/tmp/b/sdtm"), adam_dir=Path("/tmp/b/adam"), stats_dir=Path("/tmp/b/stats"), results_path=Path("/tmp/b/stats/results.json"))
```
  </action>
  <verify>
Run: `cd /Users/sanmaysarada/omni-ai-agents && python -c "from omni_agents.pipeline.resolution import ResolutionLoop; print('ResolutionLoop importable')"`

Run: `cd /Users/sanmaysarada/omni-ai-agents && python -m pytest tests/test_pipeline/test_resolution.py -v`
  </verify>
  <done>ResolutionLoop class exists with resolve, _diagnose, _generate_hint, _rerun_from_stage, _recompare_stage, _pick_best_track methods. Unit tests pass for diagnosis logic (fewer rows = failing track, ambiguous defaults to track_b), hint generation (correct stage, discrepancies, suggested checks), and pick_best_track (ambiguous defaults to track_a).</done>
</task>

<task type="auto">
  <name>Task 2: Integrate stage comparison and resolution into orchestrator run()</name>
  <files>src/omni_agents/pipeline/orchestrator.py</files>
  <action>
Update the `run()` method in `src/omni_agents/pipeline/orchestrator.py` to replace the ConsensusJudge comparison with StageComparator + ResolutionLoop.

**Add imports** at the top of orchestrator.py:
```python
from omni_agents.models.resolution import StageComparisonResult, TrackResult
from omni_agents.pipeline.stage_comparator import StageComparator
from omni_agents.pipeline.resolution import ResolutionLoop
```

Keep the existing ConsensusJudge import for now (it's still used for the verdict model).

**Replace Step 3 (Consensus gate) in `run()`:**

Remove the old ConsensusJudge.compare / compare_symmetric call. Replace with:

```python
# === Step 3: Stage-by-stage comparison (replaces ConsensusJudge) ===
consensus_dir = output_dir / "consensus"
consensus_dir.mkdir(parents=True, exist_ok=True)

if self.callback:
    self.callback.on_step_start("stage_comparison", "StageComparator", "shared")
t0 = time.monotonic()

comparison_result = StageComparator.compare_all_stages(
    track_a_result, track_b_result, self.settings.trial.n_subjects
)

# Save stage comparisons
stage_comparisons_path = consensus_dir / "stage_comparisons.json"
stage_comparisons_path.write_text(comparison_result.model_dump_json(indent=2))

duration = time.monotonic() - t0
if self.callback:
    self.callback.on_step_complete("stage_comparison", duration, 1)

# === Step 3b: Resolution loop (if disagreement detected) ===
resolution_result = None
if comparison_result.has_disagreement and self.settings.resolution.enabled:
    first_disagreement = comparison_result.first_disagreement
    logger.warning(
        f"Stage disagreement at {first_disagreement.stage}: "
        f"{first_disagreement.issues}"
    )

    resolution_loop = ResolutionLoop(
        max_iterations=self.settings.resolution.max_iterations
    )

    if self.callback:
        self.callback.on_resolution_start(
            first_disagreement.stage, 1, self.settings.resolution.max_iterations
        )

    resolution_result = await resolution_loop.resolve(
        disagreement=first_disagreement,
        track_a_result=track_a_result,
        track_b_result=track_b_result,
        orchestrator=self,
        expected_subjects=self.settings.trial.n_subjects,
    )

    if self.callback:
        self.callback.on_resolution_complete(
            first_disagreement.stage,
            resolution_result.resolved,
            resolution_result.iterations,
        )

    # Save resolution log
    resolution_log_path = consensus_dir / "resolution_log.json"
    resolution_log_path.write_text(resolution_result.model_dump_json(indent=2))

    if not resolution_result.resolved:
        if resolution_result.winning_track is None:
            # No winner -- HALT
            logger.error("Resolution failed: no winning track. Pipeline HALT.")
            state.status = "failed"
            state.save(state_path)
            raise ConsensusHaltError(
                ConsensusVerdict(
                    verdict=Verdict.HALT,
                    comparisons=[],
                    boundary_warnings=[],
                    investigation_hints=[
                        f"Stage {first_disagreement.stage} disagreement unresolved "
                        f"after {resolution_result.iterations} resolution iterations. "
                        f"Resolution log: {resolution_result.resolution_log}"
                    ],
                )
            )
        else:
            # Winner chosen but still disagree -- WARNING
            logger.warning(
                f"Resolution picked {resolution_result.winning_track} as winner "
                f"after {resolution_result.iterations} iterations"
            )

elif comparison_result.has_disagreement and not self.settings.resolution.enabled:
    # Resolution disabled -- HALT on disagreement
    first_disagreement = comparison_result.first_disagreement
    logger.error(
        f"Stage disagreement at {first_disagreement.stage} and resolution "
        f"is disabled. Pipeline HALT."
    )
    state.status = "failed"
    state.save(state_path)
    raise ConsensusHaltError(
        ConsensusVerdict(
            verdict=Verdict.HALT,
            comparisons=[],
            boundary_warnings=[],
            investigation_hints=[
                f"Stage {first_disagreement.stage} disagreement. "
                f"Resolution disabled. Issues: {first_disagreement.issues}"
            ],
        )
    )

# Build verdict for Medical Writer
if comparison_result.has_disagreement and resolution_result and resolution_result.winning_track:
    overall_verdict = Verdict.WARNING
    investigation_hints = [
        f"Resolution selected {resolution_result.winning_track} after "
        f"{resolution_result.iterations} iterations at stage {resolution_result.stage}"
    ]
else:
    overall_verdict = Verdict.PASS
    investigation_hints = []

verdict = ConsensusVerdict(
    verdict=overall_verdict,
    comparisons=[],  # Stage comparisons are in stage_comparisons.json
    boundary_warnings=[],
    investigation_hints=investigation_hints,
)

# Save verdict
verdict_path = consensus_dir / "verdict.json"
verdict_path.write_text(verdict.model_dump_json(indent=2))
logger.info(f"Pipeline verdict: {verdict.verdict.value}")

# Record step state
state.steps["consensus"] = StepState(
    name="consensus",
    agent_type="StageComparator",
    track="shared",
    status=StepStatus.COMPLETED,
    attempts=[
        StepResult(
            success=True,
            output=f"Verdict: {verdict.verdict.value}",
            attempt=1,
            duration_seconds=0,
        )
    ],
)
state.current_step = "consensus"
state.save(state_path)

# Handle HALT verdict
if verdict.verdict == Verdict.HALT:
    state.status = "failed"
    state.save(state_path)
    raise ConsensusHaltError(verdict)
```

**Update Medical Writer step:**

The Medical Writer step already reads from `consensus_dir / "verdict.json"` via volume mount. No change needed to the Medical Writer agent call itself -- it still gets `verdict_path` via its context. The verdict.json format is compatible (same Pydantic model).

However, determine which track's stats to use for the Medical Writer. Currently it uses Track A's stats:
```python
stats_dir = output_dir / "track_a" / "stats"
```

With resolution, if the winning track is track_b, we should use track_b's stats. Update:
```python
# Use winner's stats for Medical Writer (default Track A)
if resolution_result and resolution_result.winning_track:
    stats_dir = output_dir / resolution_result.winning_track / "stats"
else:
    stats_dir = output_dir / "track_a" / "stats"
```

**Remove old ConsensusJudge.compare_symmetric call** (if it was added in Plan 03). The full Stage Comparison + Resolution replaces it.

You can optionally keep `ConsensusJudge.compare_symmetric` in consensus.py for backward compat, but remove its usage from the orchestrator's run().
  </action>
  <verify>
Run: `cd /Users/sanmaysarada/omni-ai-agents && python -c "from omni_agents.pipeline.orchestrator import PipelineOrchestrator; print('Orchestrator imports cleanly with resolution integration')"`

Run: `cd /Users/sanmaysarada/omni-ai-agents && python -m pytest tests/ -v` -- all tests pass

Run: `cd /Users/sanmaysarada/omni-ai-agents && python -c "
from omni_agents.pipeline.orchestrator import PipelineOrchestrator
import inspect
src = inspect.getsource(PipelineOrchestrator.run)
assert 'StageComparator' in src, 'StageComparator not found in run()'
assert 'ResolutionLoop' in src, 'ResolutionLoop not found in run()'
assert 'stage_comparisons.json' in src, 'stage_comparisons.json not saved'
print('Integration verified')
"`
  </verify>
  <done>Orchestrator run() method: (1) runs both tracks via _run_track in parallel, (2) calls StageComparator.compare_all_stages, (3) saves stage_comparisons.json, (4) enters ResolutionLoop when disagreement detected and resolution enabled, (5) saves resolution_log.json, (6) builds verdict (PASS/WARNING/HALT), (7) saves verdict.json, (8) uses winning track's stats for Medical Writer. ConsensusJudge.compare_symmetric removed from run flow.</done>
</task>

</tasks>

<verification>
1. `python -c "from omni_agents.pipeline.resolution import ResolutionLoop"` -- importable
2. `python -m pytest tests/test_pipeline/test_resolution.py -v` -- resolution tests pass
3. `python -m pytest tests/ -v` -- all tests pass, no regressions
4. `grep -n "StageComparator" src/omni_agents/pipeline/orchestrator.py` -- present in run()
5. `grep -n "ResolutionLoop" src/omni_agents/pipeline/orchestrator.py` -- present in run()
6. `grep -n "stage_comparisons.json" src/omni_agents/pipeline/orchestrator.py` -- saved to consensus dir
7. `grep -n "resolution_log.json" src/omni_agents/pipeline/orchestrator.py` -- saved when resolution triggers
</verification>

<success_criteria>
- ResolutionLoop._diagnose identifies failing track based on deterministic rules
- ResolutionLoop._generate_hint produces structured hints with stage-appropriate suggested checks
- ResolutionLoop.resolve iterates up to max_iterations (2) and returns ResolutionResult
- Orchestrator run() uses StageComparator.compare_all_stages (not ConsensusJudge.compare)
- Disagreement triggers ResolutionLoop when resolution.enabled=True
- Disagreement without resolution enabled triggers HALT
- Resolution metadata saved to consensus/resolution_log.json
- Stage comparisons saved to consensus/stage_comparisons.json
- Medical Writer uses winning track's stats when resolution picks a winner
- All tests pass
</success_criteria>

<output>
After completion, create `.planning/phases/01-symmetric-double-programming/01-04-SUMMARY.md`
</output>
