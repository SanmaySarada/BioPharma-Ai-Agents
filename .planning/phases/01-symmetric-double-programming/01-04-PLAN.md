---
phase: 01-symmetric-double-programming
plan: 04
type: execute
wave: 3
depends_on: ["01-02", "01-03"]
files_modified:
  - src/omni_agents/pipeline/resolution.py
  - src/omni_agents/pipeline/orchestrator.py
  - tests/test_pipeline/test_resolution.py
autonomous: true

must_haves:
  truths:
    - "When tracks disagree at a stage, the resolution loop diagnoses which track erred using deterministic rules"
    - "Resolution provides structured hints (discrepancies + validation failures + suggested checks) to the failing track"
    - "Failed track retries from the disagreeing stage, and all downstream stages are also re-run (cascade: SDTM re-run triggers ADaM + Stats re-run; ADaM re-run triggers Stats re-run)"
    - "Resolution is bounded at max 2 iterations to prevent infinite loops"
    - "When tracks agree after resolution, pipeline proceeds with PASS verdict"
    - "When resolution exhausts iterations, pipeline HALTs or uses best-validated track with WARNING"
    - "Stage-by-stage comparison (post-hoc, after both tracks complete in parallel) replaces ConsensusJudge in the orchestrator run() method -- this is Strategy C from research, NOT stage-gated barriers"
    - "Resolution metadata (iterations, stage, resolved status) is saved to consensus/resolution_log.json"
    - "Resolution hint injection uses the existing previous_error/make_retry_context mechanism -- all three agents (SDTMAgent, ADaMAgent, StatsAgent) already handle previous_error in their build_user_prompt methods"
  artifacts:
    - path: "src/omni_agents/pipeline/resolution.py"
      provides: "ResolutionLoop class with resolve, _diagnose, _generate_hint, _rerun_from_stage methods"
      exports: ["ResolutionLoop"]
    - path: "src/omni_agents/pipeline/orchestrator.py"
      provides: "Orchestrator with stage comparison and resolution loop integrated into run()"
      contains: "StageComparator"
    - path: "tests/test_pipeline/test_resolution.py"
      provides: "Unit tests for ResolutionLoop diagnosis, hint generation, and agent previous_error contract"
      min_lines: 80
  key_links:
    - from: "src/omni_agents/pipeline/resolution.py"
      to: "src/omni_agents/pipeline/stage_comparator.py"
      via: "calls StageComparator for re-comparison after retry"
      pattern: "StageComparator"
    - from: "src/omni_agents/pipeline/resolution.py"
      to: "src/omni_agents/models/resolution.py"
      via: "uses ResolutionHint, ResolutionResult, StageComparison"
      pattern: "from omni_agents.models.resolution import"
    - from: "src/omni_agents/pipeline/orchestrator.py"
      to: "src/omni_agents/pipeline/stage_comparator.py"
      via: "calls StageComparator.compare_all_stages after parallel tracks complete"
      pattern: "StageComparator.compare_all_stages"
    - from: "src/omni_agents/pipeline/orchestrator.py"
      to: "src/omni_agents/pipeline/resolution.py"
      via: "calls ResolutionLoop.resolve when disagreement detected"
      pattern: "ResolutionLoop.*resolve"
---

<objective>
Build the adversarial resolution loop and integrate stage-by-stage comparison + resolution into the orchestrator's run() method.

Purpose: This is the capstone plan that makes the symmetric architecture operational. After this plan, the pipeline: (1) runs both tracks in parallel, (2) compares outputs at every stage POST-HOC (Strategy C from research -- not stage-gated barriers), (3) diagnoses which track erred when they disagree, (4) retries the failing track with targeted hints, cascading downstream re-runs, and (5) produces a final verdict with resolution metadata.

Output: New `pipeline/resolution.py` with ResolutionLoop class; updated orchestrator with stage comparison + resolution replacing ConsensusJudge; unit tests for resolution logic.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/01-symmetric-double-programming/01-RESEARCH.md

# Prior plan artifacts (all three):
@src/omni_agents/models/resolution.py
@src/omni_agents/pipeline/stage_comparator.py
@src/omni_agents/pipeline/orchestrator.py

# Referenced:
@src/omni_agents/pipeline/schema_validator.py
@src/omni_agents/pipeline/consensus.py
@src/omni_agents/pipeline/retry.py
@src/omni_agents/display/callbacks.py
@src/omni_agents/config.py
@src/omni_agents/agents/base.py
@src/omni_agents/agents/sdtm.py
@src/omni_agents/agents/adam.py
@src/omni_agents/agents/stats.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create ResolutionLoop class</name>
  <files>
    src/omni_agents/pipeline/resolution.py
    tests/test_pipeline/test_resolution.py
  </files>
  <action>
Create `src/omni_agents/pipeline/resolution.py` with the `ResolutionLoop` class.

Module docstring: "Adversarial resolution loop for semantic disagreements between symmetric tracks."

**Imports:**
```python
from __future__ import annotations

import json
from pathlib import Path
from typing import TYPE_CHECKING

from loguru import logger

from omni_agents.models.resolution import (
    ResolutionHint,
    ResolutionResult,
    StageComparison,
    TrackResult,
)
from omni_agents.pipeline.stage_comparator import StageComparator

if TYPE_CHECKING:
    from omni_agents.pipeline.orchestrator import PipelineOrchestrator
```

**Class: ResolutionLoop**

```python
class ResolutionLoop:
    """Orchestrate adversarial resolution when symmetric tracks disagree.

    Resolution protocol:
    1. DETECT: StageComparator finds disagreement at stage S
    2. DIAGNOSE: Deterministic validation rules check which track erred
    3. HINT: Generate structured hint for failing track
    4. RETRY: Re-run failing track from stage S WITH CASCADING downstream re-runs
    5. RE-COMPARE: Compare new output with other track
    6. TERMINATE: After max iterations, pick best track or HALT
    """

    def __init__(self, max_iterations: int = 2) -> None:
        self.max_iterations = max_iterations
```

**Method: `async def resolve(...) -> ResolutionResult`**

Parameters:
- `disagreement: StageComparison` -- the stage that disagreed
- `track_a_result: TrackResult` -- current Track A result
- `track_b_result: TrackResult` -- current Track B result
- `orchestrator: PipelineOrchestrator` -- reference to orchestrator for re-running tracks
- `expected_subjects: int` -- for schema validation on re-comparison

Logic:
```
resolution_log = []
current_disagreement = disagreement

for iteration in range(1, self.max_iterations + 1):
    # 1. Diagnose which track is more likely wrong
    failing_track = self._diagnose(current_disagreement, track_a_result, track_b_result)
    resolution_log.append(f"Iteration {iteration}: diagnosed {failing_track} as likely failing")

    # 2. Generate hint
    hint = self._generate_hint(current_disagreement, failing_track)
    resolution_log.append(f"Hint generated for {failing_track}: {len(hint.discrepancies)} discrepancies")

    # 3. Re-run failing track from disagreeing stage WITH CASCADE
    if failing_track == "track_a":
        track_a_result = await self._rerun_from_stage(
            track_a_result, current_disagreement.stage, hint, orchestrator
        )
    else:
        track_b_result = await self._rerun_from_stage(
            track_b_result, current_disagreement.stage, hint, orchestrator
        )

    # 4. Re-compare ALL stages (not just the disagreeing one, since cascade re-ran downstream)
    new_comparison_result = StageComparator.compare_all_stages(
        track_a_result, track_b_result, expected_subjects
    )

    if not new_comparison_result.has_disagreement:
        resolution_log.append(f"Iteration {iteration}: all stages now agree")
        return ResolutionResult(
            resolved=True, iterations=iteration, winning_track=None,
            stage=current_disagreement.stage, resolution_log=resolution_log,
        )

    # Still disagreeing -- update for next iteration
    current_disagreement = new_comparison_result.first_disagreement
    resolution_log.append(f"Iteration {iteration}: still disagreeing at {current_disagreement.stage}")

# Max iterations reached
best_track = self._pick_best_track(current_disagreement)
resolution_log.append(f"Max iterations reached. Best track: {best_track or 'NONE (HALT)'}")
return ResolutionResult(
    resolved=False, iterations=self.max_iterations, winning_track=best_track,
    stage=current_disagreement.stage, resolution_log=resolution_log,
)
```

**Method: `_diagnose(disagreement, track_a_result, track_b_result) -> str`**

Returns "track_a" or "track_b" -- which track is more likely wrong.

Deterministic diagnosis (priority order from research):
1. **Schema validation score:** For the disagreeing stage, check if one track's output has more issues in the comparison. The track with MORE issues in StageComparison is more likely wrong.
2. **Consistency with expected values:** If one track's summary shows unexpected values (e.g., n_rows != expected_subjects for SDTM), that track is wrong.
3. **Default:** If ambiguous, default to "track_b" (GPT-4o is the newer, less tested track in this pipeline). This is a heuristic tiebreaker.

For simplicity in V1: parse the issues list from the StageComparison. Check if issues mention specific tracks. If issues mention counts where one track has a value further from the expected, that track is failing. If truly ambiguous, default to "track_b".

Implementation approach:
```python
def _diagnose(self, disagreement: StageComparison, track_a_result: TrackResult, track_b_result: TrackResult) -> str:
    """Diagnose which track is more likely wrong based on deterministic rules."""
    # Parse summaries for expected value deviation
    a_summary = disagreement.track_a_summary
    b_summary = disagreement.track_b_summary

    # Heuristic: if one track has fewer rows/subjects than expected, it dropped data
    for key in ("dm_rows", "n_rows", "subjects"):
        a_val = a_summary.get(key)
        b_val = b_summary.get(key)
        if a_val is not None and b_val is not None and a_val != b_val:
            # The track with fewer items likely dropped data
            if a_val < b_val:
                return "track_a"
            return "track_b"

    # Default: assume track_b (secondary LLM) is more likely to have issues
    logger.info("Diagnosis ambiguous, defaulting to track_b as failing")
    return "track_b"
```

**Method: `_generate_hint(disagreement, failing_track) -> ResolutionHint`**

Build a ResolutionHint from the StageComparison:
- `stage`: from disagreement.stage
- `discrepancies`: from disagreement.issues
- `validation_failures`: empty list for V1 (future: run SchemaValidator and collect failures)
- `suggested_checks`: derive from stage type:
  - sdtm: ["Check deduplication logic", "Verify all subjects from raw data are included", "Check CDISC controlled terminology mapping"]
  - adam: ["Check event definition (SBP < 120 threshold)", "Verify all SDTM subjects flow through", "Check CNSR derivation logic"]
  - stats: ["Check Cox model specification", "Verify log-rank test parameters", "Check KM estimation method"]

**Method: `async def _rerun_from_stage(track_result, stage, hint, orchestrator) -> TrackResult`**

Re-runs a single stage for a track with the resolution hint injected into the agent context, THEN CASCADES to re-run all downstream stages.

**CRITICAL -- Downstream cascade logic:**
- If `stage == "sdtm"`: re-run SDTM with hint, then re-run ADaM (no hint, fresh), then re-run Stats (no hint, fresh)
- If `stage == "adam"`: re-run ADaM with hint, then re-run Stats (no hint, fresh)
- If `stage == "stats"`: re-run Stats with hint only (no downstream)

Without cascading, downstream stages remain stale -- they were computed from the OLD output of the re-run stage. For example, if SDTM is re-run and produces different DM.csv/VS.csv, the existing ADaM was built from the old DM.csv/VS.csv and is now invalid.

**Resolution hint injection mechanism:**

The resolution hint is passed via the existing `previous_error` context key. This works because:
- `BaseAgent.make_retry_context()` in `base.py` sets `context["previous_error"]` and `context["attempt_number"]`
- ALL three agents (`SDTMAgent`, `ADaMAgent`, `StatsAgent`) check for `"previous_error" in context` in their `build_user_prompt()` method and include it as error feedback in the prompt
- The `ResolutionHint.to_prompt_text()` output is structured enough to serve as meaningful error feedback

For the HINT stage (the stage being resolved), inject via previous_error:
```python
context = {
    "input_path": "/workspace/input/SBPdata.csv",
    "output_dir": "/workspace",
    "previous_error": hint.to_prompt_text(),
    "attempt_number": 1,  # Resolution attempt
}
```

For CASCADE stages (downstream re-runs), use normal context WITHOUT previous_error (they get fresh inputs from the re-run upstream stage).

**Complete implementation:**

```python
async def _rerun_from_stage(
    self,
    track_result: TrackResult,
    stage: str,
    hint: ResolutionHint,
    orchestrator: PipelineOrchestrator,
) -> TrackResult:
    """Re-run from the failing stage and cascade through all downstream stages.

    Cascade order:
    - sdtm: re-run sdtm (with hint) -> adam (fresh) -> stats (fresh)
    - adam: re-run adam (with hint) -> stats (fresh)
    - stats: re-run stats (with hint) only
    """
    track_id = track_result.track_id

    # Determine which LLM to use
    if track_id == "track_a":
        from omni_agents.llm.gemini import GeminiAdapter
        llm = GeminiAdapter(orchestrator.settings.llm.gemini)
    else:
        from omni_agents.llm.openai_adapter import OpenAIAdapter
        llm = OpenAIAdapter(orchestrator.settings.llm.openai)

    prompt_dir = Path(__file__).parent.parent / "templates" / "prompts"
    raw_dir = track_result.sdtm_dir.parent.parent / "raw"

    stages_to_run = []
    if stage == "sdtm":
        stages_to_run = ["sdtm", "adam", "stats"]
    elif stage == "adam":
        stages_to_run = ["adam", "stats"]
    elif stage == "stats":
        stages_to_run = ["stats"]
    else:
        raise ValueError(f"Unknown stage: {stage}")

    logger.info(f"Resolution re-run for {track_id}: {' -> '.join(stages_to_run)}")

    for run_stage in stages_to_run:
        # Only the FIRST stage (the one that disagreed) gets the hint
        is_hint_stage = (run_stage == stage)

        if run_stage == "sdtm":
            from omni_agents.agents.sdtm import SDTMAgent
            agent = SDTMAgent(llm=llm, prompt_dir=prompt_dir, trial_config=orchestrator.settings.trial)
            context = {
                "input_path": "/workspace/input/SBPdata.csv",
                "output_dir": "/workspace",
            }
            if is_hint_stage:
                context["previous_error"] = hint.to_prompt_text()
                context["attempt_number"] = 1
            await orchestrator._run_agent(
                agent=agent, context=context, work_dir=track_result.sdtm_dir,
                input_volumes={str(raw_dir): "/workspace/input"},
                expected_inputs=["/workspace/input/SBPdata.csv"],
                expected_outputs=["DM.csv", "VS.csv"],
                track_id=track_id,
            )
            from omni_agents.pipeline.schema_validator import SchemaValidator
            SchemaValidator.validate_sdtm(track_result.sdtm_dir, orchestrator.settings.trial.n_subjects)

        elif run_stage == "adam":
            from omni_agents.agents.adam import ADaMAgent
            agent = ADaMAgent(llm=llm, prompt_dir=prompt_dir, trial_config=orchestrator.settings.trial)
            context = {
                "input_dir": "/workspace/input",
                "output_dir": "/workspace",
            }
            if is_hint_stage:
                context["previous_error"] = hint.to_prompt_text()
                context["attempt_number"] = 1
            await orchestrator._run_agent(
                agent=agent, context=context, work_dir=track_result.adam_dir,
                input_volumes={str(track_result.sdtm_dir): "/workspace/input"},
                expected_inputs=["DM.csv", "VS.csv"],
                expected_outputs=["ADTTE.rds", "ADTTE_summary.json"],
                track_id=track_id,
            )
            from omni_agents.pipeline.schema_validator import SchemaValidator
            SchemaValidator.validate_adam(track_result.adam_dir, orchestrator.settings.trial.n_subjects)

        elif run_stage == "stats":
            from omni_agents.agents.stats import StatsAgent
            agent = StatsAgent(llm=llm, prompt_dir=prompt_dir, trial_config=orchestrator.settings.trial)
            context = {
                "adam_dir": "/workspace/adam",
                "sdtm_dir": "/workspace/sdtm",
                "output_dir": "/workspace",
            }
            if is_hint_stage:
                context["previous_error"] = hint.to_prompt_text()
                context["attempt_number"] = 1
            await orchestrator._run_agent(
                agent=agent, context=context, work_dir=track_result.stats_dir,
                input_volumes={
                    str(track_result.adam_dir): "/workspace/adam",
                    str(track_result.sdtm_dir): "/workspace/sdtm",
                },
                expected_inputs=["ADTTE.rds", "DM.csv"],
                expected_outputs=["results.json", "km_plot.png"],
                track_id=track_id,
            )
            from omni_agents.pipeline.schema_validator import SchemaValidator
            SchemaValidator.validate_stats(track_result.stats_dir)

    # Return the same TrackResult object -- the directories are the same,
    # but their CONTENTS have been updated by the re-runs
    return track_result
```

Note: Every `orchestrator._run_agent()` call passes `track_id=track_id` for cache key isolation (matching the pattern from Plan 01-03).

**Method: `_recompare_stage(stage, track_a_result, track_b_result, expected_subjects) -> StageComparison`**

Dispatches to the appropriate StageComparator method:
```python
def _recompare_stage(self, stage: str, track_a_result: TrackResult, track_b_result: TrackResult, expected_subjects: int) -> StageComparison:
    if stage == "sdtm":
        return StageComparator.compare_sdtm(track_a_result.sdtm_dir, track_b_result.sdtm_dir, expected_subjects)
    elif stage == "adam":
        return StageComparator.compare_adam(track_a_result.adam_dir, track_b_result.adam_dir, expected_subjects)
    elif stage == "stats":
        return StageComparator.compare_stats(track_a_result.stats_dir, track_b_result.stats_dir)
    else:
        raise ValueError(f"Unknown stage: {stage}")
```

Note: `_recompare_stage` is kept as a utility but the main `resolve()` loop now uses `StageComparator.compare_all_stages()` for full re-comparison after cascade.

**Method: `_pick_best_track(disagreement) -> str | None`**

If one track has clearly fewer issues (fewer items in issues list), return that track_id.
If ambiguous (equal issues), return None (triggers HALT).

For V1: return "track_a" as default winner when ambiguous (Gemini is the established track).

---

**Tests** (`tests/test_pipeline/test_resolution.py`):

Test the synchronous, deterministic parts of ResolutionLoop (diagnosis, hint generation, pick_best_track, cascade logic). Do NOT test the async resolve() method -- that requires mocking the full orchestrator.

```python
class TestDiagnose:
    def test_diagnose_fewer_rows_track_a(self):
        """Track with fewer rows is diagnosed as failing."""
        loop = ResolutionLoop(max_iterations=2)
        disagreement = StageComparison(
            stage="sdtm", matches=False,
            issues=["DM row count: track_a=298, track_b=300"],
            track_a_summary={"dm_rows": 298, "vs_rows": 7748},
            track_b_summary={"dm_rows": 300, "vs_rows": 7800},
        )
        # Track A has fewer rows -> Track A is likely wrong
        result = loop._diagnose(disagreement, mock_track_a, mock_track_b)
        assert result == "track_a"

    def test_diagnose_fewer_rows_track_b(self):
        """Track B with fewer rows is diagnosed as failing."""
        loop = ResolutionLoop(max_iterations=2)
        disagreement = StageComparison(
            stage="sdtm", matches=False,
            issues=["DM row count: track_a=300, track_b=295"],
            track_a_summary={"dm_rows": 300, "vs_rows": 7800},
            track_b_summary={"dm_rows": 295, "vs_rows": 7670},
        )
        result = loop._diagnose(disagreement, mock_track_a, mock_track_b)
        assert result == "track_b"

    def test_diagnose_ambiguous_defaults_track_b(self):
        """When diagnosis is ambiguous, default to track_b."""
        loop = ResolutionLoop(max_iterations=2)
        disagreement = StageComparison(
            stage="stats", matches=False,
            issues=["logrank_p differs: 0.031 vs 0.037"],
            track_a_summary={"logrank_p": 0.031},
            track_b_summary={"logrank_p": 0.037},
        )
        result = loop._diagnose(disagreement, mock_track_a, mock_track_b)
        assert result == "track_b"

class TestGenerateHint:
    def test_hint_structure(self):
        """Hint contains stage, discrepancies, and suggested checks."""
        loop = ResolutionLoop()
        disagreement = StageComparison(
            stage="sdtm", matches=False,
            issues=["DM row count: 298 vs 300", "Subject ID mismatch"],
            track_a_summary={}, track_b_summary={},
        )
        hint = loop._generate_hint(disagreement, "track_b")
        assert hint.stage == "sdtm"
        assert len(hint.discrepancies) == 2
        assert len(hint.suggested_checks) > 0
        text = hint.to_prompt_text()
        assert "RESOLUTION HINT" in text
        assert "DM row count" in text

    def test_hint_suggested_checks_per_stage(self):
        """Each stage gets stage-appropriate suggested checks."""
        loop = ResolutionLoop()
        for stage in ("sdtm", "adam", "stats"):
            disagreement = StageComparison(
                stage=stage, matches=False, issues=["some issue"],
                track_a_summary={}, track_b_summary={},
            )
            hint = loop._generate_hint(disagreement, "track_b")
            assert hint.stage == stage
            assert len(hint.suggested_checks) >= 2

class TestPickBestTrack:
    def test_ambiguous_returns_track_a(self):
        """When ambiguous, default winner is track_a."""
        loop = ResolutionLoop()
        disagreement = StageComparison(
            stage="stats", matches=False,
            issues=["some issue"],
            track_a_summary={"logrank_p": 0.031},
            track_b_summary={"logrank_p": 0.037},
        )
        result = loop._pick_best_track(disagreement)
        assert result == "track_a"

class TestCascadeLogic:
    def test_sdtm_cascades_to_adam_and_stats(self):
        """When stage is sdtm, _rerun_from_stage should re-run sdtm, adam, stats."""
        # Verify the cascade ordering logic by checking stages_to_run
        # This tests the internal logic without async orchestrator calls
        loop = ResolutionLoop()
        # Test stage ordering logic directly
        for stage, expected_cascade in [
            ("sdtm", ["sdtm", "adam", "stats"]),
            ("adam", ["adam", "stats"]),
            ("stats", ["stats"]),
        ]:
            if stage == "sdtm":
                stages = ["sdtm", "adam", "stats"]
            elif stage == "adam":
                stages = ["adam", "stats"]
            elif stage == "stats":
                stages = ["stats"]
            assert stages == expected_cascade, f"Stage {stage} cascade mismatch"

class TestAgentPreviousErrorContract:
    """Verify that all pipeline agents handle previous_error in build_user_prompt.

    This is CRITICAL for resolution hint injection. The resolution loop passes
    hints as previous_error context. If any agent ignores previous_error, the
    hint is silently dropped and retry produces the same output.
    """

    def test_sdtm_agent_handles_previous_error(self):
        """SDTMAgent.build_user_prompt includes previous_error when present."""
        from omni_agents.agents.sdtm import SDTMAgent
        # SDTMAgent requires llm and prompt_dir but build_user_prompt is a pure method
        # that only reads from context dict
        import inspect
        source = inspect.getsource(SDTMAgent.build_user_prompt)
        assert "previous_error" in source, (
            "SDTMAgent.build_user_prompt does not reference previous_error. "
            "Resolution hints will be silently dropped."
        )

    def test_adam_agent_handles_previous_error(self):
        """ADaMAgent.build_user_prompt includes previous_error when present."""
        from omni_agents.agents.adam import ADaMAgent
        import inspect
        source = inspect.getsource(ADaMAgent.build_user_prompt)
        assert "previous_error" in source, (
            "ADaMAgent.build_user_prompt does not reference previous_error. "
            "Resolution hints will be silently dropped."
        )

    def test_stats_agent_handles_previous_error(self):
        """StatsAgent.build_user_prompt includes previous_error when present."""
        from omni_agents.agents.stats import StatsAgent
        import inspect
        source = inspect.getsource(StatsAgent.build_user_prompt)
        assert "previous_error" in source, (
            "StatsAgent.build_user_prompt does not reference previous_error. "
            "Resolution hints will be silently dropped."
        )
```

Use mock TrackResult objects (just need track_id, can use dummy Paths):
```python
from pathlib import Path
mock_track_a = TrackResult(track_id="track_a", sdtm_dir=Path("/tmp/a/sdtm"), adam_dir=Path("/tmp/a/adam"), stats_dir=Path("/tmp/a/stats"), results_path=Path("/tmp/a/stats/results.json"))
mock_track_b = TrackResult(track_id="track_b", sdtm_dir=Path("/tmp/b/sdtm"), adam_dir=Path("/tmp/b/adam"), stats_dir=Path("/tmp/b/stats"), results_path=Path("/tmp/b/stats/results.json"))
```
  </action>
  <verify>
Run: `cd /Users/sanmaysarada/omni-ai-agents && python -c "from omni_agents.pipeline.resolution import ResolutionLoop; print('ResolutionLoop importable')"`

Run: `cd /Users/sanmaysarada/omni-ai-agents && python -m pytest tests/test_pipeline/test_resolution.py -v`
  </verify>
  <done>ResolutionLoop class exists with resolve, _diagnose, _generate_hint, _rerun_from_stage, _recompare_stage, _pick_best_track methods. _rerun_from_stage cascades downstream re-runs (sdtm triggers adam+stats, adam triggers stats). All _run_agent calls pass track_id=track_id. Unit tests pass for diagnosis logic, hint generation, cascade ordering, pick_best_track, and the critical agent previous_error contract (all three agents verified to handle previous_error).</done>
</task>

<task type="auto">
  <name>Task 2: Integrate stage comparison and resolution into orchestrator run()</name>
  <files>src/omni_agents/pipeline/orchestrator.py</files>
  <action>
Update the `run()` method in `src/omni_agents/pipeline/orchestrator.py` to replace the ConsensusJudge comparison with StageComparator + ResolutionLoop.

**Add imports** at the top of orchestrator.py:
```python
from omni_agents.models.resolution import StageComparisonResult, TrackResult
from omni_agents.pipeline.stage_comparator import StageComparator
from omni_agents.pipeline.resolution import ResolutionLoop
```

Keep the existing ConsensusJudge import for now (it's still used for the verdict model).

**Replace Step 3 (Consensus gate) in `run()`:**

Remove the old ConsensusJudge.compare / compare_symmetric call. Replace with:

```python
# === Step 3: Stage-by-stage comparison (post-hoc, Strategy C from research) ===
# Both tracks have completed in parallel. Now compare outputs at every stage.
# This is NOT stage-gated -- both tracks ran all stages independently.
consensus_dir = output_dir / "consensus"
consensus_dir.mkdir(parents=True, exist_ok=True)

if self.callback:
    self.callback.on_step_start("stage_comparison", "StageComparator", "shared")
t0 = time.monotonic()

comparison_result = StageComparator.compare_all_stages(
    track_a_result, track_b_result, self.settings.trial.n_subjects
)

# Save stage comparisons
stage_comparisons_path = consensus_dir / "stage_comparisons.json"
stage_comparisons_path.write_text(comparison_result.model_dump_json(indent=2))

duration = time.monotonic() - t0
if self.callback:
    self.callback.on_step_complete("stage_comparison", duration, 1)

# === Step 3b: Resolution loop (if disagreement detected) ===
resolution_result = None
if comparison_result.has_disagreement and self.settings.resolution.enabled:
    first_disagreement = comparison_result.first_disagreement
    logger.warning(
        f"Stage disagreement at {first_disagreement.stage}: "
        f"{first_disagreement.issues}"
    )

    resolution_loop = ResolutionLoop(
        max_iterations=self.settings.resolution.max_iterations
    )

    if self.callback:
        self.callback.on_resolution_start(
            first_disagreement.stage, 1, self.settings.resolution.max_iterations
        )

    resolution_result = await resolution_loop.resolve(
        disagreement=first_disagreement,
        track_a_result=track_a_result,
        track_b_result=track_b_result,
        orchestrator=self,
        expected_subjects=self.settings.trial.n_subjects,
    )

    if self.callback:
        self.callback.on_resolution_complete(
            first_disagreement.stage,
            resolution_result.resolved,
            resolution_result.iterations,
        )

    # Save resolution log
    resolution_log_path = consensus_dir / "resolution_log.json"
    resolution_log_path.write_text(resolution_result.model_dump_json(indent=2))

    if not resolution_result.resolved:
        if resolution_result.winning_track is None:
            # No winner -- HALT
            logger.error("Resolution failed: no winning track. Pipeline HALT.")
            state.status = "failed"
            state.save(state_path)
            raise ConsensusHaltError(
                ConsensusVerdict(
                    verdict=Verdict.HALT,
                    comparisons=[],
                    boundary_warnings=[],
                    investigation_hints=[
                        f"Stage {first_disagreement.stage} disagreement unresolved "
                        f"after {resolution_result.iterations} resolution iterations. "
                        f"Resolution log: {resolution_result.resolution_log}"
                    ],
                )
            )
        else:
            # Winner chosen but still disagree -- WARNING
            logger.warning(
                f"Resolution picked {resolution_result.winning_track} as winner "
                f"after {resolution_result.iterations} iterations"
            )

elif comparison_result.has_disagreement and not self.settings.resolution.enabled:
    # Resolution disabled -- HALT on disagreement
    first_disagreement = comparison_result.first_disagreement
    logger.error(
        f"Stage disagreement at {first_disagreement.stage} and resolution "
        f"is disabled. Pipeline HALT."
    )
    state.status = "failed"
    state.save(state_path)
    raise ConsensusHaltError(
        ConsensusVerdict(
            verdict=Verdict.HALT,
            comparisons=[],
            boundary_warnings=[],
            investigation_hints=[
                f"Stage {first_disagreement.stage} disagreement. "
                f"Resolution disabled. Issues: {first_disagreement.issues}"
            ],
        )
    )

# Build verdict for Medical Writer
if comparison_result.has_disagreement and resolution_result and resolution_result.winning_track:
    overall_verdict = Verdict.WARNING
    investigation_hints = [
        f"Resolution selected {resolution_result.winning_track} after "
        f"{resolution_result.iterations} iterations at stage {resolution_result.stage}"
    ]
else:
    overall_verdict = Verdict.PASS
    investigation_hints = []

verdict = ConsensusVerdict(
    verdict=overall_verdict,
    comparisons=[],  # Stage comparisons are in stage_comparisons.json
    boundary_warnings=[],
    investigation_hints=investigation_hints,
)

# Save verdict
verdict_path = consensus_dir / "verdict.json"
verdict_path.write_text(verdict.model_dump_json(indent=2))
logger.info(f"Pipeline verdict: {verdict.verdict.value}")

# Record step state
state.steps["consensus"] = StepState(
    name="consensus",
    agent_type="StageComparator",
    track="shared",
    status=StepStatus.COMPLETED,
    attempts=[
        StepResult(
            success=True,
            output=f"Verdict: {verdict.verdict.value}",
            attempt=1,
            duration_seconds=0,
        )
    ],
)
state.current_step = "consensus"
state.save(state_path)

# Handle HALT verdict
if verdict.verdict == Verdict.HALT:
    state.status = "failed"
    state.save(state_path)
    raise ConsensusHaltError(verdict)
```

**Update Medical Writer step:**

The Medical Writer step already reads from `consensus_dir / "verdict.json"` via volume mount. No change needed to the Medical Writer agent call itself -- it still gets `verdict_path` via its context. The verdict.json format is compatible (same Pydantic model).

However, determine which track's stats to use for the Medical Writer. Currently it uses Track A's stats:
```python
stats_dir = output_dir / "track_a" / "stats"
```

With resolution, if the winning track is track_b, we should use track_b's stats. Update:
```python
# Use winner's stats for Medical Writer (default Track A)
if resolution_result and resolution_result.winning_track:
    stats_dir = output_dir / resolution_result.winning_track / "stats"
else:
    stats_dir = output_dir / "track_a" / "stats"
```

**Remove old ConsensusJudge.compare_symmetric call** (if it was added in Plan 03). The full Stage Comparison + Resolution replaces it.

You can optionally keep `ConsensusJudge.compare_symmetric` in consensus.py for backward compat, but remove its usage from the orchestrator's run().
  </action>
  <verify>
Run: `cd /Users/sanmaysarada/omni-ai-agents && python -c "from omni_agents.pipeline.orchestrator import PipelineOrchestrator; print('Orchestrator imports cleanly with resolution integration')"`

Run: `cd /Users/sanmaysarada/omni-ai-agents && python -m pytest tests/ -v` -- all tests pass

Run: `cd /Users/sanmaysarada/omni-ai-agents && python -c "
from omni_agents.pipeline.orchestrator import PipelineOrchestrator
import inspect
src = inspect.getsource(PipelineOrchestrator.run)
assert 'StageComparator' in src, 'StageComparator not found in run()'
assert 'ResolutionLoop' in src, 'ResolutionLoop not found in run()'
assert 'stage_comparisons.json' in src, 'stage_comparisons.json not saved'
assert 'Strategy C' in src or 'post-hoc' in src, 'Missing post-hoc/Strategy C comment'
print('Integration verified')
"`
  </verify>
  <done>Orchestrator run() method: (1) runs both tracks via _run_track in parallel, (2) calls StageComparator.compare_all_stages POST-HOC (Strategy C -- not stage-gated), (3) saves stage_comparisons.json, (4) enters ResolutionLoop when disagreement detected and resolution enabled, (5) resolution cascades downstream re-runs, (6) saves resolution_log.json, (7) builds verdict (PASS/WARNING/HALT), (8) saves verdict.json, (9) uses winning track's stats for Medical Writer. ConsensusJudge.compare_symmetric removed from run flow.</done>
</task>

</tasks>

<verification>
1. `python -c "from omni_agents.pipeline.resolution import ResolutionLoop"` -- importable
2. `python -m pytest tests/test_pipeline/test_resolution.py -v` -- resolution tests pass (including agent previous_error contract tests)
3. `python -m pytest tests/ -v` -- all tests pass, no regressions
4. `grep -n "StageComparator" src/omni_agents/pipeline/orchestrator.py` -- present in run()
5. `grep -n "ResolutionLoop" src/omni_agents/pipeline/orchestrator.py` -- present in run()
6. `grep -n "stage_comparisons.json" src/omni_agents/pipeline/orchestrator.py` -- saved to consensus dir
7. `grep -n "resolution_log.json" src/omni_agents/pipeline/orchestrator.py` -- saved when resolution triggers
8. `grep -n "previous_error" src/omni_agents/pipeline/resolution.py` -- hint injected via previous_error
</verification>

<success_criteria>
- ResolutionLoop._diagnose identifies failing track based on deterministic rules
- ResolutionLoop._generate_hint produces structured hints with stage-appropriate suggested checks
- ResolutionLoop.resolve iterates up to max_iterations (2) and returns ResolutionResult
- ResolutionLoop._rerun_from_stage cascades downstream re-runs (sdtm triggers adam+stats, adam triggers stats)
- Resolution hints injected via previous_error context key (verified by agent contract tests)
- All three agents (SDTMAgent, ADaMAgent, StatsAgent) confirmed to handle previous_error in build_user_prompt
- Orchestrator run() uses StageComparator.compare_all_stages post-hoc (Strategy C, not stage-gated)
- Disagreement triggers ResolutionLoop when resolution.enabled=True
- Disagreement without resolution enabled triggers HALT
- Resolution metadata saved to consensus/resolution_log.json
- Stage comparisons saved to consensus/stage_comparisons.json
- Medical Writer uses winning track's stats when resolution picks a winner
- All tests pass
</success_criteria>

<output>
After completion, create `.planning/phases/01-symmetric-double-programming/01-04-SUMMARY.md`
</output>
