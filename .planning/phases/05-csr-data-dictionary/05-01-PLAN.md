---
phase: 05-csr-data-dictionary
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/omni_agents/pipeline/data_dictionary.py
  - src/omni_agents/templates/prompts/medical_writer.j2
  - src/omni_agents/pipeline/orchestrator.py
  - src/omni_agents/pipeline/schema_validator.py
  - tests/test_pipeline/test_data_dictionary.py
  - tests/test_pipeline/test_schema_validator.py
autonomous: true

must_haves:
  truths:
    - "CSR Word document no longer contains a data dictionary section (Section 8 removed from prompt template)"
    - "sdtm/data_dictionary.csv exists in each track's sdtm/ output directory with correct CDISC variable definitions"
    - "adam/data_dictionary.csv exists in each track's adam/ output directory with correct ADTTE variable definitions"
    - "Data dictionaries are generated deterministically by Python (no LLM call, no Docker execution)"
    - "Schema validator rejects output directories missing data_dictionary.csv"
  artifacts:
    - path: "src/omni_agents/pipeline/data_dictionary.py"
      provides: "Deterministic data dictionary CSV generation for SDTM and ADaM domains"
      exports: ["write_sdtm_data_dictionary", "write_adam_data_dictionary"]
      min_lines: 60
    - path: "src/omni_agents/templates/prompts/medical_writer.j2"
      provides: "Medical writer prompt template WITHOUT Section 8 data dictionary"
      contains: "Do NOT include a data dictionary"
    - path: "src/omni_agents/pipeline/orchestrator.py"
      provides: "Orchestrator calling data dictionary generation in _run_track()"
    - path: "src/omni_agents/pipeline/schema_validator.py"
      provides: "Schema validator checking for data_dictionary.csv in SDTM and ADaM directories"
    - path: "tests/test_pipeline/test_data_dictionary.py"
      provides: "Tests for data dictionary generation"
      min_lines: 60
    - path: "tests/test_pipeline/test_schema_validator.py"
      provides: "Updated tests for schema validator data dictionary checks"
  key_links:
    - from: "src/omni_agents/pipeline/orchestrator.py"
      to: "src/omni_agents/pipeline/data_dictionary.py"
      via: "import and call after SDTM/ADaM validation in _run_track()"
      pattern: "from omni_agents.pipeline.data_dictionary import"
    - from: "src/omni_agents/pipeline/schema_validator.py"
      to: "data_dictionary.csv file"
      via: "file existence check in validate_sdtm() and validate_adam()"
      pattern: "data_dictionary.csv"
    - from: "src/omni_agents/pipeline/data_dictionary.py"
      to: "src/omni_agents/config.py"
      via: "TrialConfig parameter for event_threshold"
      pattern: "TrialConfig"
---

<objective>
Create deterministic data dictionary CSV files for SDTM and ADaM output directories, remove the data dictionary section from the CSR Word document template, wire generation into the orchestrator, and add schema validator checks.

Purpose: Data dictionary metadata belongs alongside the data files it describes, not embedded in a narrative CSR document. This follows CDISC best practices and makes variable definitions machine-readable. The change also shortens the CSR and removes an anti-pattern flagged in regulatory contexts.

Output: Two new Python functions that generate `data_dictionary.csv` files deterministically (no LLM, no Docker), a modified medical writer template without Section 8, orchestrator integration, schema validator checks, and comprehensive tests.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/research/FEATURES.md
@.planning/research/ARCHITECTURE.md
@.planning/research/PITFALLS.md

# Source files to modify
@src/omni_agents/pipeline/orchestrator.py
@src/omni_agents/pipeline/schema_validator.py
@src/omni_agents/templates/prompts/medical_writer.j2
@src/omni_agents/config.py
@src/omni_agents/models/schemas.py

# Test patterns
@tests/test_pipeline/test_schema_validator.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create data_dictionary.py module and modify medical_writer.j2 template</name>
  <files>src/omni_agents/pipeline/data_dictionary.py, src/omni_agents/templates/prompts/medical_writer.j2</files>
  <action>
**Part A: Create `src/omni_agents/pipeline/data_dictionary.py`**

Create a new module with two public functions. These generate deterministic CSV data dictionary files -- NO LLM call, NO Docker execution. The content is static CDISC domain knowledge parameterized only by `TrialConfig` fields.

```python
"""Deterministic data dictionary generation for SDTM and ADaM outputs.

Writes CSV data dictionary files alongside data files in each track's output
directories. Content is static CDISC domain knowledge parameterized by
TrialConfig -- no LLM or Docker execution needed (DICT-04).
"""

import csv
from pathlib import Path

from omni_agents.config import TrialConfig


def write_sdtm_data_dictionary(sdtm_dir: Path, trial_config: TrialConfig) -> Path:
    """Write SDTM variable definitions to sdtm/data_dictionary.csv.

    Covers DM and VS domain variables produced by the SDTM agent.

    Args:
        sdtm_dir: Track's sdtm/ output directory (e.g., output/track_a/sdtm/).
        trial_config: Trial configuration for parameterized derivation text.

    Returns:
        Path to the written data_dictionary.csv file.
    """
    ...


def write_adam_data_dictionary(adam_dir: Path, trial_config: TrialConfig) -> Path:
    """Write ADaM ADTTE variable definitions to adam/data_dictionary.csv.

    Covers ADTTE dataset variables produced by the ADaM agent.

    Args:
        adam_dir: Track's adam/ output directory (e.g., output/track_a/adam/).
        trial_config: Trial configuration for event_threshold in derivation text.

    Returns:
        Path to the written data_dictionary.csv file.
    """
    ...
```

**CSV format**: Columns are `Variable`, `Label`, `Type`, `Derivation`. Write using `csv.DictWriter` with these exact column names as the header row.

**SDTM data dictionary rows** (for DM.csv and VS.csv variables). Use the variable definitions from the SDTM prompt template (`sdtm.j2`) as the source of truth:

DM domain variables:
- STUDYID | Study Identifier | Char | Fixed value assigned to all subjects
- DOMAIN | Domain Abbreviation | Char | Fixed: "DM"
- USUBJID | Unique Subject Identifier | Char | Study ID prefix + subject number
- SUBJID | Subject Identifier | Char | Original subject identifier from raw data
- AGE | Age at Baseline | Num | Carried from raw data, integer years
- AGEU | Age Units | Char | Fixed: "YEARS"
- SEX | Sex | Char | Carried from raw data (M/F); CDISC CT
- RACE | Race | Char | Mapped to CDISC controlled terminology from raw data
- ARM | Planned Arm | Char | Treatment arm from randomization (Treatment/Placebo)
- ARMCD | Planned Arm Code | Char | Derived from ARM: TRT or PBO
- ACTARM | Actual Arm | Char | Same as ARM for this study
- ACTARMCD | Actual Arm Code | Char | Same as ARMCD for this study

VS domain variables:
- STUDYID | Study Identifier | Char | Fixed value assigned to all subjects
- DOMAIN | Domain Abbreviation | Char | Fixed: "VS"
- USUBJID | Unique Subject Identifier | Char | From DM domain
- VSSEQ | Sequence Number | Num | Row number within each subject, starting at 1
- VSTESTCD | Vital Signs Test Short Name | Char | Fixed: "SYSBP" (CDISC controlled term)
- VSTEST | Vital Signs Test Name | Char | Fixed: "Systolic Blood Pressure"
- VSORRES | Result in Original Units | Char | SBP value as character; empty string for missing
- VSSTRESN | Result in Standard Units (Numeric) | Num | SBP value as numeric; NA for missing
- VSSTRESU | Standard Units | Char | Fixed: "mmHg"
- VISITNUM | Visit Number | Num | Visit number (0 through {trial_config.visits - 1})
- VISIT | Visit Name | Char | "Screening" for Visit 0, "Week N" for subsequent visits
- VSBLFL | Baseline Flag | Char | "Y" for Visit 0 (baseline), empty for others

**ADaM data dictionary rows** (for ADTTE dataset). Use the variable definitions from the ADaM prompt template (`adam.j2`) and the current Section 8 of `medical_writer.j2`:

- STUDYID | Study Identifier | Char | Fixed study identifier
- USUBJID | Unique Subject Identifier | Char | From DM domain
- PARAMCD | Parameter Code | Char | Fixed: "TTESB120"
- PARAM | Parameter Description | Char | "Time to First SBP Below {trial_config.endpoint} Threshold"
- AVAL | Analysis Value | Num | Time to event in weeks (VISITNUM of event or censor point)
- CNSR | Censoring Flag | Num | 0 = event occurred, 1 = censored
- STARTDT | Time-to-Event Start Date | Num | 0 (baseline, Week 0)
- EVNTDESC | Event Description | Char | "SBP < {event_threshold} mmHg" for events; "Dropout" or "End of study" for censored
- AGE | Age at Baseline | Num | Carried from DM domain via left_join on USUBJID
- SEX | Sex | Char | Carried from DM domain via left_join on USUBJID
- ARM | Planned Arm | Char | Carried from DM domain via left_join on USUBJID
- ARMCD | Planned Arm Code | Char | Carried from DM domain via left_join on USUBJID

Where `{event_threshold}` is computed as: `int(trial_config.treatment_sbp_mean)` (this is consistent with how the event_threshold is derived elsewhere -- the treatment SBP mean IS the event threshold for this trial design, matching the `{{ event_threshold }}` Jinja2 variable in `adam.j2`).

**IMPORTANT**: Check how `event_threshold` is provided to the templates. Look at the SDTM and ADaM agent classes (`agents/sdtm.py`, `agents/adam.py`) and their `get_system_prompt_vars()` methods to see how `event_threshold` is computed. Use the same derivation. If it is simply `int(trial_config.treatment_sbp_mean)`, use that. If a different calculation exists, match it exactly.

Each function:
1. Builds a list of dicts (one per variable, keys: Variable, Label, Type, Derivation)
2. Opens `{dir}/data_dictionary.csv` for writing
3. Uses `csv.DictWriter` with fieldnames `["Variable", "Label", "Type", "Derivation"]`
4. Writes header row + all data rows
5. Returns the Path to the written file

**Part B: Modify `src/omni_agents/templates/prompts/medical_writer.j2`**

Remove Section 8 entirely (lines 172-211 in the current template). This is the block starting with:
```
### Section 8: Data Dictionary -- ADTTE Variables (CSR-04)
```
and ending just before:
```
## Save Document
```

Also remove Critical Rule 6 (line 234 in current template):
```
6. **DATA DICTIONARY (CSR-04)**: Include the data dictionary section with ADTTE variable derivations as specified above. This is required regulatory content explaining how analysis variables were derived from source data.
```

After removing Section 8, add a negative instruction in its place (PITFALL-06 mitigation -- the LLM may still generate a data dictionary if not explicitly told not to):

```
### Note: No Data Dictionary Section

Do NOT include a data dictionary or variable derivations section in the CSR. The data dictionary is generated as a separate standalone file (data_dictionary.csv) alongside the data files. The CSR should end after Figure 1.
```

Renumber the Critical Rules (the current Rule 7 becomes Rule 6 after removing the data dictionary rule). The final rule list should be:
1. CROSS-REFERENCES (CSR-03) -- unchanged
2. DISPLAY VALUES ONLY -- unchanged
3. COLUMN SELECTION -- unchanged
4. IMAGE DIMENSIONS -- unchanged
5. WARNING HANDLING -- unchanged
6. TABLE1 CSV -- unchanged (was previously Rule 7)

Verify the "## Save Document" section remains intact after Section 8 removal.
  </action>
  <verify>
`python -c "from omni_agents.pipeline.data_dictionary import write_sdtm_data_dictionary, write_adam_data_dictionary; print('imports OK')"` -- module imports successfully.

`python -c "
from pathlib import Path
import tempfile
from omni_agents.config import TrialConfig
from omni_agents.pipeline.data_dictionary import write_sdtm_data_dictionary, write_adam_data_dictionary
tc = TrialConfig()
with tempfile.TemporaryDirectory() as d:
    sd = Path(d) / 'sdtm'; sd.mkdir()
    ad = Path(d) / 'adam'; ad.mkdir()
    sp = write_sdtm_data_dictionary(sd, tc)
    ap = write_adam_data_dictionary(ad, tc)
    assert sp.exists(), 'SDTM dict not written'
    assert ap.exists(), 'ADaM dict not written'
    print(f'SDTM: {sp} ({sp.stat().st_size} bytes)')
    print(f'ADaM: {ap} ({ap.stat().st_size} bytes)')
    # Check CSV structure
    import csv
    with open(sp) as f:
        reader = csv.DictReader(f)
        rows = list(reader)
        assert set(reader.fieldnames) == {'Variable', 'Label', 'Type', 'Derivation'}, f'Bad headers: {reader.fieldnames}'
        assert len(rows) >= 20, f'Expected 20+ SDTM vars, got {len(rows)}'
    with open(ap) as f:
        reader = csv.DictReader(f)
        rows = list(reader)
        assert len(rows) >= 10, f'Expected 10+ ADaM vars, got {len(rows)}'
    print('All checks passed')
"` -- CSV files generated with correct structure.

Verify `medical_writer.j2` no longer contains "Section 8" or "Data Dictionary" or "CSR-04" or "dict_data":
`grep -c "Section 8\|Data Dictionary\|CSR-04\|dict_data" src/omni_agents/templates/prompts/medical_writer.j2` -- should return 0 matches (the only reference should be the negative instruction "Do NOT include a data dictionary").

Verify negative instruction is present:
`grep "Do NOT include a data dictionary" src/omni_agents/templates/prompts/medical_writer.j2` -- should match.
  </verify>
  <done>
`data_dictionary.py` module exists with `write_sdtm_data_dictionary()` and `write_adam_data_dictionary()` functions that generate correct CSV files deterministically. `medical_writer.j2` Section 8 is fully removed, Critical Rule 6 (CSR-04) is removed, negative instruction is added, and remaining rules are renumbered. No references to "dict_data" or "CSR-04" remain in the template.
  </done>
</task>

<task type="auto">
  <name>Task 2: Wire into orchestrator, add schema validator checks, and write tests</name>
  <files>src/omni_agents/pipeline/orchestrator.py, src/omni_agents/pipeline/schema_validator.py, tests/test_pipeline/test_data_dictionary.py, tests/test_pipeline/test_schema_validator.py</files>
  <action>
**Part A: Wire data dictionary generation into orchestrator._run_track()**

In `src/omni_agents/pipeline/orchestrator.py`:

1. Add import at the top of the file with other pipeline imports:
```python
from omni_agents.pipeline.data_dictionary import write_adam_data_dictionary, write_sdtm_data_dictionary
```

2. In the `_run_track()` method, add the SDTM data dictionary write AFTER SDTM schema validation passes (after line 319: `logger.info(f"SDTM schema validation passed ({track_id})")`):

```python
        # Generate SDTM data dictionary (DICT-02, DICT-04)
        write_sdtm_data_dictionary(sdtm_dir, self.settings.trial)
        logger.info(f"SDTM data dictionary written ({track_id})")
```

3. Add the ADaM data dictionary write AFTER ADaM schema validation passes (after line 349: `logger.info(f"ADaM schema validation passed ({track_id})")`):

```python
        # Generate ADaM data dictionary (DICT-03, DICT-04)
        write_adam_data_dictionary(adam_dir, self.settings.trial)
        logger.info(f"ADaM data dictionary written ({track_id})")
```

These are synchronous Python calls (no async needed -- they just write a CSV file). They run inside the existing track flow, so both Track A and Track B get their own copy. The calls go AFTER schema validation to ensure we only write data dictionaries for valid outputs.

**Part B: Add schema validator checks for data_dictionary.csv (DICT-05)**

In `src/omni_agents/pipeline/schema_validator.py`:

1. In `validate_sdtm()`, AFTER all existing validation checks pass (just before the final `logger.info` at the end of the method), add a check for data_dictionary.csv. However, the data dictionary is written AFTER validation in the orchestrator, so the validator cannot check it during the same validation call. Instead, add a new classmethod:

```python
    @classmethod
    def validate_output_completeness(cls, track_dir: Path) -> None:
        """Validate that all expected output artifacts exist in a track directory.

        Called after all pipeline steps complete for a track. Checks for
        data dictionary files alongside SDTM and ADaM outputs (DICT-05).

        Args:
            track_dir: Root track directory (e.g., output/track_a/).

        Raises:
            SchemaValidationError: If expected output files are missing.
        """
        issues: list[str] = []

        sdtm_dict = track_dir / "sdtm" / "data_dictionary.csv"
        if not sdtm_dict.exists():
            issues.append(
                "sdtm/data_dictionary.csv not found — SDTM data dictionary missing"
            )

        adam_dict = track_dir / "adam" / "data_dictionary.csv"
        if not adam_dict.exists():
            issues.append(
                "adam/data_dictionary.csv not found — ADaM data dictionary missing"
            )

        if issues:
            raise SchemaValidationError("OutputCompleteness", issues)

        logger.info("Output completeness check passed: data dictionaries present")
```

2. In `orchestrator.py`, in the `_run_track()` method, add a call to `validate_output_completeness` at the very end of `_run_track()`, AFTER the Stats step and BEFORE the `return TrackResult(...)` statement (after line 383: `logger.info(f"Stats schema validation passed ({track_id})")`):

```python
        # Validate all output artifacts are present (DICT-05)
        SchemaValidator.validate_output_completeness(track_dir)
        logger.info(f"Output completeness check passed ({track_id})")
```

**Part C: Write tests for data dictionary generation**

Create `tests/test_pipeline/test_data_dictionary.py`:

```python
"""Tests for deterministic data dictionary generation (DICT-02, DICT-03, DICT-04)."""

import csv
from pathlib import Path

import pytest

from omni_agents.config import TrialConfig
from omni_agents.pipeline.data_dictionary import (
    write_adam_data_dictionary,
    write_sdtm_data_dictionary,
)
```

Test cases:

1. `test_sdtm_data_dictionary_creates_file(tmp_path)` -- Call `write_sdtm_data_dictionary(tmp_path, TrialConfig())`. Assert `(tmp_path / "data_dictionary.csv").exists()`.

2. `test_sdtm_data_dictionary_has_correct_columns(tmp_path)` -- Write dict, read back with `csv.DictReader`. Assert fieldnames are exactly `["Variable", "Label", "Type", "Derivation"]`.

3. `test_sdtm_data_dictionary_contains_dm_and_vs_variables(tmp_path)` -- Write dict, read rows. Assert that variables include at least: STUDYID, USUBJID, AGE, SEX, RACE, ARM, VSTESTCD, VSTEST, VSSTRESN. This verifies both DM and VS domain variables are present.

4. `test_adam_data_dictionary_creates_file(tmp_path)` -- Call `write_adam_data_dictionary(tmp_path, TrialConfig())`. Assert file exists.

5. `test_adam_data_dictionary_has_correct_columns(tmp_path)` -- Same column check as SDTM.

6. `test_adam_data_dictionary_contains_adtte_variables(tmp_path)` -- Assert variables include: USUBJID, PARAMCD, PARAM, AVAL, CNSR, STARTDT, EVNTDESC, AGE, SEX, ARM.

7. `test_adam_data_dictionary_uses_event_threshold(tmp_path)` -- Create `TrialConfig(treatment_sbp_mean=130.0)`. Write ADaM dict. Read CSV and verify that at least one Derivation cell contains "130" (the event threshold). This proves the dictionary is parameterized, not hardcoded.

8. `test_sdtm_data_dictionary_returns_path(tmp_path)` -- Assert the returned Path matches `tmp_path / "data_dictionary.csv"`.

9. `test_data_dictionary_is_deterministic(tmp_path)` -- Write SDTM dict twice to two different dirs. Read both CSVs and assert they are identical (same rows in same order). This proves DICT-04.

10. `test_adam_data_dictionary_derivation_text_not_empty(tmp_path)` -- Write ADaM dict, read rows. Assert that every row's "Derivation" field is non-empty (no blank derivations).

**Part D: Update schema validator tests**

In `tests/test_pipeline/test_schema_validator.py`, add tests for the new `validate_output_completeness` method:

11. `test_output_completeness_passes_with_both_dicts(tmp_path)` -- Create `track_dir/sdtm/data_dictionary.csv` and `track_dir/adam/data_dictionary.csv` (can be minimal content). Call `SchemaValidator.validate_output_completeness(track_dir)`. Should not raise.

12. `test_output_completeness_fails_missing_sdtm_dict(tmp_path)` -- Create only `track_dir/adam/data_dictionary.csv`. Call `validate_output_completeness`. Should raise `SchemaValidationError` matching "sdtm/data_dictionary.csv not found".

13. `test_output_completeness_fails_missing_adam_dict(tmp_path)` -- Create only `track_dir/sdtm/data_dictionary.csv`. Call `validate_output_completeness`. Should raise `SchemaValidationError` matching "adam/data_dictionary.csv not found".

14. `test_output_completeness_fails_missing_both(tmp_path)` -- Create empty track_dir with sdtm/ and adam/ subdirectories but no data_dictionary.csv files. Should raise with 2 issues.

Add the import for `SchemaValidationError` at the top if not already present.
  </action>
  <verify>
`cd /Users/sanmaysarada/omni-ai-agents && python -m pytest tests/test_pipeline/test_data_dictionary.py -v` -- all 10 data dictionary tests pass.

`cd /Users/sanmaysarada/omni-ai-agents && python -m pytest tests/test_pipeline/test_schema_validator.py -v` -- all schema validator tests pass (old + new).

`cd /Users/sanmaysarada/omni-ai-agents && python -m pytest tests/ -v` -- full test suite passes, no regressions.

Verify orchestrator import:
`python -c "from omni_agents.pipeline.orchestrator import PipelineOrchestrator; print('OK')"` -- no import errors.
  </verify>
  <done>
Orchestrator `_run_track()` calls `write_sdtm_data_dictionary()` after SDTM validation and `write_adam_data_dictionary()` after ADaM validation. Schema validator has `validate_output_completeness()` that checks for data_dictionary.csv in both sdtm/ and adam/ directories. 14+ tests pass covering: file creation, CSV column structure, variable coverage for both domains, event threshold parameterization, determinism, output completeness validation (pass + fail cases). Full test suite has no regressions.
  </done>
</task>

</tasks>

<verification>
1. `python -m pytest tests/test_pipeline/test_data_dictionary.py tests/test_pipeline/test_schema_validator.py -v` -- all data dictionary and schema validator tests pass
2. `python -m pytest tests/ -v` -- full test suite passes, no regressions
3. `grep -c "dict_data\|CSR-04" src/omni_agents/templates/prompts/medical_writer.j2` returns 0 -- Section 8 fully removed
4. `grep "Do NOT include a data dictionary" src/omni_agents/templates/prompts/medical_writer.j2` matches -- negative instruction present
5. `grep "write_sdtm_data_dictionary\|write_adam_data_dictionary" src/omni_agents/pipeline/orchestrator.py` matches -- orchestrator integration present
6. `grep "validate_output_completeness" src/omni_agents/pipeline/schema_validator.py` matches -- validator check exists
7. Manual: data_dictionary.csv files contain Variable, Label, Type, Derivation columns with all expected SDTM and ADaM variables
</verification>

<success_criteria>
- DICT-01: medical_writer.j2 has no Section 8, no CSR-04 rule, has negative instruction preventing LLM from generating data dictionary content
- DICT-02: write_sdtm_data_dictionary() generates sdtm/data_dictionary.csv with DM and VS variable definitions (12 DM vars + 12 VS vars = 24 rows)
- DICT-03: write_adam_data_dictionary() generates adam/data_dictionary.csv with ADTTE variable definitions (12 rows), parameterized by event_threshold from TrialConfig
- DICT-04: Both functions are pure Python with csv.DictWriter -- no LLM call, no Docker, no async. Called by orchestrator after schema validation in _run_track()
- DICT-05: SchemaValidator.validate_output_completeness() checks for data_dictionary.csv in both sdtm/ and adam/ directories, raises SchemaValidationError if missing
- All tests pass, no regressions in existing 117-test suite
</success_criteria>

<output>
After completion, create `.planning/phases/05-csr-data-dictionary/05-01-SUMMARY.md`
</output>
